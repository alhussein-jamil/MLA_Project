\subsection{Hyperparameter Tuning}

The optimization of our model's performance involved meticulous adjustments to specific hyperparameters. The following configurations were explored and refined during the fine-tuning process:

\begin{enumerate}
    \item \textbf{Batch Size:} Experimentation with batch sizes ranging from 80 to 256 was conducted during training. Smaller batches proved to induce chaotic behavior in some cases when using SGD.
    
    \item \textbf{Optimizer:} For Adam, the initial learning rate was set to $0.001$, while Adadelta utilized a learning rate of $1.0$, $\epsilon=10^{-6}$, and $\rho=0.95$.
    
    \item \textbf{Dataset Size:} After shuffling, we limit the enormous dataset to around 4 million translation examples, equivalent to approximately 60 million words. Additionally, we limit our expirements to $T_x=T_y=15$ as the original paper shows significant divergence at this point when it comes to bleu score. 
    
    \item \textbf{Dropout Layers:} Dropout layers with a 20\% probability were incorporated into the decoder RNN module and the Fully Connected layer used for maxout.
    
    \item \textbf{Batch Normalization:} A single layer with the number of features equal to $T_x$ was added at the output of the encoder.
    
    \item \textbf{Gradient Clipping:} All gradients were clipped to have a norm at most 1.0, following the approach outlined in the paper.
    
    \item \textbf{L2 Regularization:} To address loss explosions, especially on the university's GPU, L2 regularization was introduced, mitigating numerical precision errors not observed on local GPUs.
\end{enumerate}

\subsection{Training Process Overview}

The training process involved several crucial components:

\subsection{Experimental Setup}

The following experimental setup was used for training and evaluation:

\begin{enumerate}
    \item \textbf{Evaluation Frequency:} The model was evaluated on a complete dataset of 3000 samples. Four randomly chosen examples were translated using the beam search algorithm. A threshold on log probabilities (-2.8) was used to control the generation process, stopping the generation of next words if the maximum log probability dropped below the threshold.
    
    \item \textbf{Hardware:} Training was performed on both local hardware with Windows 11 and an Nvidia RTX 3080 (8GB VRAM) portable GPU, as well as the university's Linux servers equipped with Nvidia Quadro RTX 6000 (24GB VRAM).
    
    \item \textbf{Epochs:} For smaller databases (around 100K examples), approximately 20 epochs were executed. The larger dataset of 4 million examples required an extended training period of 6 epochs to capture complexity and patterns comprehensively.
    
    \item \textbf{Training Time:} Each model was trained for approximately 20 hours to achieve the final results.
\end{enumerate}

\subsection{Metrics}

The cost function employed was Negative-Log-Loss, producing initial losses around 10 and concluding at approximately 2 during training. Additionally, the Bleu score, which assesses repetitions of sequences of a fixed length $k$ (with $k=3$ in our experiments), was used to compare phrases.


\subsection{Results}

This section presents the outcomes of our endeavor to reimplement a noteworthy paper. Due to some vague details, adaptations were made by adding or removing functionalities. Two models, RNNsearch-20 and RNNEncDeco-20, were trained for around 6 epochs. The first figure compares the translation performance of the two models for a phrase of size 20, evaluating the mean Bleu score for both translations and the expected one.

\subsection{Discussion}

During the implementation of our enhanced encoder-decoder model with attention mechanism, we encountered several challenges and limitations that affected our results.

One major challenge was the issue of overfitting due to the use of a smaller vocabulary size and a limited dataset. This resulted in significant deviations from the results reported in the original paper. We attempted to address this problem by adjusting various components of the model, such as reducing the embedding size and modifying the hidden size of the output network. However, finding the optimal combination of hyperparameters proved to be a time-consuming process.

Another limitation of our implementation was the lack of data selection, as described in the original paper. Due to time constraints, we were unable to implement this crucial step, which could have significantly improved our results by extracting more relevant data from the dataset.

Furthermore, the limited training time of only 6 epochs and the constraint of a smaller dataset also impacted our results. We were unable to train the model for longer periods or with larger datasets, which may have affected the model's ability to capture complex patterns comprehensively.

Additionally, since we implemented everything from scratch, we encountered several bugs and inconsistencies between our implementation and the details provided in the original paper. These challenges further hindered our progress and affected the accuracy of our results.

Overall, while we made efforts to enhance the basic model and address the challenges we encountered, the limitations and constraints we faced had a significant impact on the performance of our model.
