{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#auto reload \n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nadia/anaconda3/envs/projetmla/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os \n",
    "from datasets import load_dataset, load_from_disk\n",
    "import torch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    " \n",
    "bow_english = pd.read_csv(\"data/unigram_freq_en.csv\")\n",
    "bow_french = pd.read_csv(\"data/unigram_freq_fr.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_len, val_len = 10000, 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nadia/anaconda3/envs/projetmla/lib/python3.10/site-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by mode='default'.\n",
      "  table = cls._concat_blocks(blocks, axis=0)\n"
     ]
    }
   ],
   "source": [
    "wmt14 = load_dataset(\"wmt14\", \"fr-en\", data_dir=\"data/\")\n",
    "\n",
    "# Accessing example data\n",
    "train_data = wmt14[\"train\"]\n",
    "val_data = wmt14[\"validation\"]\n",
    "\n",
    "if train_len is not None:\n",
    "    train_data = train_data.select(range(train_len))\n",
    "if val_len is not None:\n",
    "    val_data = val_data.select(range(val_len))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reprise de la session  ->  Resumption of the session \n",
      "\n",
      "\n",
      "Je déclare reprise la session du Parlement européen qui avait été interrompue le vendredi 17 décembre dernier et je vous renouvelle tous mes vux en espérant que vous avez passé de bonnes vacances.  ->  I declare resumed the session of the European Parliament adjourned on Friday 17 December 1999, and I would like once again to wish you a happy new year in the hope that you enjoyed a pleasant festive period. \n",
      "\n",
      "\n",
      "Comme vous avez pu le constater, le grand \"bogue de l'an 2000\" ne s'est pas produit. En revanche, les citoyens d'un certain nombre de nos pays ont été victimes de catastrophes naturelles qui ont vraiment été terribles.  ->  Although, as you will have seen, the dreaded 'millennium bug' failed to materialise, still the people in a number of countries suffered a series of natural disasters that truly were dreadful. \n",
      "\n",
      "\n",
      "Vous avez souhaité un débat à ce sujet dans les prochains jours, au cours de cette période de session.  ->  You have requested a debate on this subject in the course of the next few days, during this part-session. \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "examples = 4 \n",
    "#check some data and their translation\n",
    "for i, t in zip(range(examples), train_data):\n",
    "    ex = t[\"translation\"]\n",
    "    print(ex[\"fr\"], \" -> \", ex[\"en\"], \"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This',\n",
       " 'is',\n",
       " 'an',\n",
       " 'example',\n",
       " 'sentence',\n",
       " 'with',\n",
       " 'words',\n",
       " 'that',\n",
       " 'may',\n",
       " 'not',\n",
       " 'be',\n",
       " 'in',\n",
       " 'the',\n",
       " 'shortlist',\n",
       " '.']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sacremoses import MosesTokenizer, MosesDetokenizer\n",
    "\n",
    "# Sample sentence\n",
    "sentence = \"This is an  example sentence with words that may not be in the shortlist.\"\n",
    "\n",
    "mt_en = MosesTokenizer(lang=\"en\")\n",
    "mt_fr = MosesTokenizer(lang=\"fr\")\n",
    "\n",
    "tokenized = mt_en.tokenize(sentence)\n",
    "tokenized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenizerWrapper:\n",
    "    def __init__(self, tokenizer_en, tokenizer_fr):\n",
    "        self.tokenizer_en = tokenizer_en\n",
    "        self.tokenizer_fr = tokenizer_fr\n",
    "    \n",
    "    def tokenize_function(self, examples):\n",
    "        return {\"tokenized_en\" : self.tokenizer_en.tokenize(examples[\"translation\"][\"en\"]),\n",
    "                \"tokenized_fr\" : self.tokenizer_fr.tokenize(examples[\"translation\"][\"fr\"])}\n",
    "    \n",
    "tokenizer_wrapper = TokenizerWrapper(mt_en, mt_fr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=19):   0%|          | 0/10000 [00:00<?, ? examples/s]/Users/nadia/anaconda3/envs/projetmla/lib/python3.10/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by mode='default'.\n",
      "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n",
      "/Users/nadia/anaconda3/envs/projetmla/lib/python3.10/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by mode='default'.\n",
      "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n",
      "/Users/nadia/anaconda3/envs/projetmla/lib/python3.10/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by mode='default'.\n",
      "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n",
      "/Users/nadia/anaconda3/envs/projetmla/lib/python3.10/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by mode='default'.\n",
      "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n",
      "/Users/nadia/anaconda3/envs/projetmla/lib/python3.10/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by mode='default'.\n",
      "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n",
      "/Users/nadia/anaconda3/envs/projetmla/lib/python3.10/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by mode='default'.\n",
      "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n",
      "/Users/nadia/anaconda3/envs/projetmla/lib/python3.10/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by mode='default'.\n",
      "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n",
      "/Users/nadia/anaconda3/envs/projetmla/lib/python3.10/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by mode='default'.\n",
      "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n",
      "/Users/nadia/anaconda3/envs/projetmla/lib/python3.10/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by mode='default'.\n",
      "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n",
      "/Users/nadia/anaconda3/envs/projetmla/lib/python3.10/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by mode='default'.\n",
      "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n",
      "/Users/nadia/anaconda3/envs/projetmla/lib/python3.10/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by mode='default'.\n",
      "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n",
      "/Users/nadia/anaconda3/envs/projetmla/lib/python3.10/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by mode='default'.\n",
      "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n",
      "/Users/nadia/anaconda3/envs/projetmla/lib/python3.10/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by mode='default'.\n",
      "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n",
      "/Users/nadia/anaconda3/envs/projetmla/lib/python3.10/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by mode='default'.\n",
      "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n",
      "/Users/nadia/anaconda3/envs/projetmla/lib/python3.10/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by mode='default'.\n",
      "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n",
      "/Users/nadia/anaconda3/envs/projetmla/lib/python3.10/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by mode='default'.\n",
      "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n",
      "/Users/nadia/anaconda3/envs/projetmla/lib/python3.10/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by mode='default'.\n",
      "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n",
      "/Users/nadia/anaconda3/envs/projetmla/lib/python3.10/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by mode='default'.\n",
      "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n",
      "/Users/nadia/anaconda3/envs/projetmla/lib/python3.10/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by mode='default'.\n",
      "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n",
      "Map (num_proc=19): 100%|██████████| 10000/10000 [00:00<00:00, 10300.34 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 1194856.28 examples/s]\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(\"processed_data/tokenized_train_data{}\".format(train_len)):\n",
    "    tokenized_train_data  = train_data.map(tokenizer_wrapper.tokenize_function, batched=False, num_proc=19, remove_columns=[\"translation\"])\n",
    "    tokenized_train_data.save_to_disk(\"processed_data/tokenized_train_data{}\".format(train_len))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=19): 100%|██████████| 1000/1000 [00:00<00:00, 4935.54 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 1000/1000 [00:00<00:00, 199671.71 examples/s]\n"
     ]
    }
   ],
   "source": [
    "    \n",
    "if not os.path.exists(\"processed_data/tokenized_val_data{}\".format(val_len)):\n",
    "    tokenized_val_data  = val_data.map(tokenizer_wrapper.tokenize_function, batched=False, num_proc=19, remove_columns=[\"translation\"])\n",
    "    tokenized_val_data.save_to_disk(\"processed_data/tokenized_val_data{}\".format(val_len))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_train_data = load_from_disk(\"processed_data/tokenized_train_data{}\".format(train_len))\n",
    "tokenized_val_data = load_from_disk(\"processed_data/tokenized_val_data{}\".format(val_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# df = pd.read_csv(\"data/Copy of Lexique383.csv\")\n",
    "# df.sort_values(by=\"9_freqfilms2\", ascending=False, inplace=True)\n",
    "# only_words_freq = df[[\"1_ortho\", \"9_freqfilms2\"]][:30000]\n",
    "# #drop index\n",
    "# only_words_freq.reset_index(drop=True, inplace=True)\n",
    "# #change column names\n",
    "# only_words_freq.columns = [\"word\", \"count\"]\n",
    "# only_words_freq.to_csv(\"data/unigram_freq_fr.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_frequent_english_words = bow_english[\"word\"][:30000].apply(lambda x: str(x)).tolist()\n",
    "most_frequent_english_words.append(\"UNK\")\n",
    "most_frequent_french_words = bow_french[\"word\"][:30000].apply(lambda x: str(x)).tolist()\n",
    "most_frequent_french_words.append(\"UNK\")\n",
    "tokenized_most_frequent_english_words = mt_en.tokenize(\" \".join(most_frequent_english_words))\n",
    "tokenized_most_frequent_french_words = mt_fr.tokenize(\" \".join(most_frequent_french_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class toIdTransform:\n",
    "    def __init__(self, most_frequent_words, tor):\n",
    "        self.most_frequent_words = most_frequent_words\n",
    "        self.word_to_id = {word: i for i, word in enumerate(most_frequent_words)}\n",
    "        self.tor = tor\n",
    "    \n",
    "    def __call__(self, tokenized):\n",
    "        return {\"ids_en\":[self.word_to_id.get(token, self.word_to_id[\"UNK\"]) for token in tokenized[\"tokenized_en\"]],\n",
    "                \"ids_fr\":[self.word_to_id.get(token, self.word_to_id[\"UNK\"]) for token in tokenized[\"tokenized_fr\"]]}\n",
    "    \n",
    "to_id_transform = toIdTransform(tokenized_most_frequent_english_words, torch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=19): 100%|██████████| 10000/10000 [00:02<00:00, 4163.20 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 867739.16 examples/s]\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(\"processed_data/id_train_data{}\".format(train_len)):\n",
    "    tokenized_train_data = tokenized_train_data.map(to_id_transform, batched=False, num_proc=19)\n",
    "    tokenized_train_data.save_to_disk(\"processed_data/id_train_data{}\".format(train_len))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=19): 100%|██████████| 1000/1000 [00:01<00:00, 598.56 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 1000/1000 [00:00<00:00, 193571.35 examples/s]\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(\"processed_data/id_val_data{}\".format(val_len)):\n",
    "    tokenized_val_data = tokenized_val_data.map(to_id_transform, batched=False, num_proc=19)\n",
    "    tokenized_val_data.save_to_disk(\"processed_data/id_val_data{}\".format(val_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_train_data = load_from_disk(\"processed_data/id_train_data{}\".format(train_len))\n",
    "tokenized_val_data = load_from_disk(\"processed_data/id_val_data{}\".format(val_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=19): 100%|██████████| 10000/10000 [00:00<00:00, 34597.88 examples/s]\n",
      "Map (num_proc=19): 100%|██████████| 1000/1000 [00:00<00:00, 6781.27 examples/s]\n"
     ]
    }
   ],
   "source": [
    "class to_tensor:\n",
    "    def __init__(self, tor):\n",
    "        self.tor = tor\n",
    "    \n",
    "    def __call__(self, ids):\n",
    "        return {\"ids_en\":self.tor.tensor(ids[\"ids_en\"]),\n",
    "                \"ids_fr\":self.tor.tensor(ids[\"ids_fr\"])}\n",
    "    \n",
    "tokenized_train_data = tokenized_train_data.map(to_tensor(torch), batched=False, num_proc=19)\n",
    "tokenized_val_data = tokenized_val_data.map(to_tensor(torch), batched=False, num_proc=19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_vector(ids, vocab_size):\n",
    "    vecs = torch.zeros(len(ids), vocab_size)\n",
    "    vecs[range(len(ids)), ids] = 1\n",
    "    return vecs\n",
    "\n",
    "class to_vec:\n",
    "    def __init__(self, tor, vocab_size, lang):\n",
    "        self.tor = tor\n",
    "        self.vocab_size = vocab_size\n",
    "        self.lang = lang\n",
    "\n",
    "    \n",
    "    def __call__(self, ids):\n",
    "        ids = self.tor.tensor(ids[\"ids_{}\".format(self.lang)])\n",
    "        vecs = self.tor.zeros(len(ids), self.vocab_size)\n",
    "        vecs[range(len(ids)), ids] = 1\n",
    "        return vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to_vec_transform_en = to_vec(torch, len(tokenized_most_frequent_english_words), \"en\")\n",
    "# to_vec_transform_fr = to_vec(torch, len(tokenized_most_frequent_french_words), \"fr\")\n",
    "# vecs_en = [\n",
    "\n",
    "# ]\n",
    "# vecs_fr = [\n",
    "\n",
    "# ]\n",
    "# for i in range(len(tokenized_train_data)):\n",
    "#     vecs_en.append(to_vec_transform_en(tokenized_train_data[i]))\n",
    "#     vecs_fr.append(to_vec_transform_fr(tokenized_train_data[i]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.fcnn import FCNN\n",
    "\n",
    "model = FCNN(len(tokenized_most_frequent_english_words), [10,10], 30000, \"cuda\" if torch.cuda.is_available() else \"cpu\",torch.nn.ReLU())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 30000])\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(tokenized_train_data)):\n",
    "\n",
    "    tensorized = torch.tensor(tokenized_train_data[i][\"ids_en\"]).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    result = model(to_vector(tensorized, len(tokenized_most_frequent_english_words)).to(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "    \n",
    "    print(result.shape)\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.rnn import RNN\n",
    "\n",
    "encoder_rnn = RNN(len(tokenized_most_frequent_english_words),100,2,\"cuda\" if torch.cuda.is_available() else \"cpu\",torch.nn.Tanh())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4, 100])\n",
      "RNN(\n",
      "  (rnn): RNN(30001, 100, num_layers=2, batch_first=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "for x in tokenized_train_data:\n",
    "    tensorized = torch.tensor(x[\"ids_en\"]).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    result = encoder_rnn(to_vector(tensorized, len(tokenized_most_frequent_english_words)).to(\"cuda\" if torch.cuda.is_available() else \"cpu\").view(1,-1,len(tokenized_most_frequent_english_words)))\n",
    "    print(result[0].shape)\n",
    "    break\n",
    "print (encoder_rnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30001\n",
      "Encoder(\n",
      "  (embedding): Embedding(30001, 256)\n",
      "  (rnn): RNN(\n",
      "    (rnn): RNN(256, 512, num_layers=2, batch_first=True, dropout=0.5)\n",
      "  )\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      ")\n",
      "tensor([30000,     1,     0,  1497])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.FloatTensor instead (while checking arguments for embedding)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/nadia/Desktop/M2 ISI/MLA/MLA_Project/main.ipynb Cellule 25\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/nadia/Desktop/M2%20ISI/MLA/MLA_Project/main.ipynb#X33sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m tensorized \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(x[\u001b[39m\"\u001b[39m\u001b[39mids_en\u001b[39m\u001b[39m\"\u001b[39m])\u001b[39m.\u001b[39mto(\u001b[39m\"\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mis_available() \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mlong()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/nadia/Desktop/M2%20ISI/MLA/MLA_Project/main.ipynb#X33sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mprint\u001b[39m(tensorized)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/nadia/Desktop/M2%20ISI/MLA/MLA_Project/main.ipynb#X33sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m result \u001b[39m=\u001b[39m encoder(to_vector(tensorized, \u001b[39mlen\u001b[39;49m(tokenized_most_frequent_english_words))\u001b[39m.\u001b[39;49mto(\u001b[39m\"\u001b[39;49m\u001b[39mcuda\u001b[39;49m\u001b[39m\"\u001b[39;49m \u001b[39mif\u001b[39;49;00m torch\u001b[39m.\u001b[39;49mcuda\u001b[39m.\u001b[39;49mis_available() \u001b[39melse\u001b[39;49;00m \u001b[39m\"\u001b[39;49m\u001b[39mcpu\u001b[39;49m\u001b[39m\"\u001b[39;49m)\u001b[39m.\u001b[39;49mview(\u001b[39m1\u001b[39;49m, \u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m, \u001b[39mlen\u001b[39;49m(tokenized_most_frequent_english_words)))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/nadia/Desktop/M2%20ISI/MLA/MLA_Project/main.ipynb#X33sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39mprint\u001b[39m(result[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mshape)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/nadia/Desktop/M2%20ISI/MLA/MLA_Project/main.ipynb#X33sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/projetmla/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/projetmla/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/M2 ISI/MLA/MLA_Project/src/models/enco_deco.py:38\u001b[0m, in \u001b[0;36mEncoder.forward\u001b[0;34m(self, source)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, source):               \n\u001b[0;32m---> 38\u001b[0m     embedded \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membedding(source))\n\u001b[1;32m     39\u001b[0m     output, hidden \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrnn(embedded)     \n\u001b[1;32m     40\u001b[0m     \u001b[39mreturn\u001b[39;00m hidden\n",
      "File \u001b[0;32m~/anaconda3/envs/projetmla/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/projetmla/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/projetmla/lib/python3.10/site-packages/torch/nn/modules/sparse.py:162\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 162\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49membedding(\n\u001b[1;32m    163\u001b[0m         \u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding_idx, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_norm,\n\u001b[1;32m    164\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnorm_type, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscale_grad_by_freq, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msparse)\n",
      "File \u001b[0;32m~/anaconda3/envs/projetmla/lib/python3.10/site-packages/torch/nn/functional.py:2233\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2227\u001b[0m     \u001b[39m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[1;32m   2228\u001b[0m     \u001b[39m# XXX: equivalent to\u001b[39;00m\n\u001b[1;32m   2229\u001b[0m     \u001b[39m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m   2230\u001b[0m     \u001b[39m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[1;32m   2231\u001b[0m     \u001b[39m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[1;32m   2232\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[39minput\u001b[39m, max_norm, norm_type)\n\u001b[0;32m-> 2233\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49membedding(weight, \u001b[39minput\u001b[39;49m, padding_idx, scale_grad_by_freq, sparse)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.FloatTensor instead (while checking arguments for embedding)"
     ]
    }
   ],
   "source": [
    "from models.rnn import RNN\n",
    "from models.enco_deco import Encoder, Decoder  \n",
    "\n",
    "# Importing necessary modules\n",
    "\n",
    "# Création de l'encodeur (Encoder creation)\n",
    "encoder = Encoder(len(tokenized_most_frequent_english_words), 256, 512, 2, \"cuda\" if torch.cuda.is_available() else \"cpu\", 0.5)\n",
    "\n",
    "# Printing the length of most frequent English words and the encoder details\n",
    "print(len(tokenized_most_frequent_english_words))\n",
    "print(encoder)\n",
    "\n",
    "# Looping through the tokenized training data\n",
    "for x in tokenized_train_data:\n",
    "    # Converting English word indices to a PyTorch tensor\n",
    "    tensorized = torch.tensor(x[\"ids_en\"]).to(\"cuda\" if torch.cuda.is_available() else \"cpu\").long()\n",
    "    print(tensorized)\n",
    "\n",
    "    # Applying the encoder to the input tensor\n",
    "    result = encoder(to_vector(tensorized, len(tokenized_most_frequent_english_words))\n",
    "                     .to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "                     .view(1, -1, len(tokenized_most_frequent_english_words)))\n",
    "    print(result[0].shape)\n",
    "    break\n",
    "\n",
    "# Création du décodeur (Decoder creation)\n",
    "decoder = Decoder(len(tokenized_most_frequent_english_words), 100, 2, \"cuda\" if torch.cuda.is_available() else \"cpu\", torch.nn.Tanh())\n",
    "\n",
    "# Using the decoder on some input\n",
    "decoded_output, hidden_state = decoder(torch.tensor([1]), result[0], None)\n",
    "\n",
    "# Printing the shapes of the decoded output and hidden state\n",
    "print(\"Decoded Output shape:\", decoded_output.shape)\n",
    "print(\"Hidden State shape:\", hidden_state.shape)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlaproject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
