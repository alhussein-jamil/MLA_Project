{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "#auto reload \n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os \n",
    "from datasets import load_dataset, load_from_disk\n",
    "import torch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    " \n",
    "bow_english = pd.read_csv(\"data/unigram_freq_en.csv\")\n",
    "bow_french = pd.read_csv(\"data/unigram_freq_fr.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_len, val_len = 10000, 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "wmt14 = load_dataset(\"wmt14\", \"fr-en\", data_dir=\"data/\")\n",
    "\n",
    "# Accessing example data\n",
    "train_data = wmt14[\"train\"]\n",
    "val_data = wmt14[\"validation\"]\n",
    "\n",
    "if train_len is not None:\n",
    "    train_data = train_data.select(range(train_len))\n",
    "if val_len is not None:\n",
    "    val_data = val_data.select(range(val_len))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reprise de la session  ->  Resumption of the session \n",
      "\n",
      "\n",
      "Je déclare reprise la session du Parlement européen qui avait été interrompue le vendredi 17 décembre dernier et je vous renouvelle tous mes vux en espérant que vous avez passé de bonnes vacances.  ->  I declare resumed the session of the European Parliament adjourned on Friday 17 December 1999, and I would like once again to wish you a happy new year in the hope that you enjoyed a pleasant festive period. \n",
      "\n",
      "\n",
      "Comme vous avez pu le constater, le grand \"bogue de l'an 2000\" ne s'est pas produit. En revanche, les citoyens d'un certain nombre de nos pays ont été victimes de catastrophes naturelles qui ont vraiment été terribles.  ->  Although, as you will have seen, the dreaded 'millennium bug' failed to materialise, still the people in a number of countries suffered a series of natural disasters that truly were dreadful. \n",
      "\n",
      "\n",
      "Vous avez souhaité un débat à ce sujet dans les prochains jours, au cours de cette période de session.  ->  You have requested a debate on this subject in the course of the next few days, during this part-session. \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "examples = 4 \n",
    "#check some data and their translation\n",
    "for i, t in zip(range(examples), train_data):\n",
    "    ex = t[\"translation\"]\n",
    "    print(ex[\"fr\"], \" -> \", ex[\"en\"], \"\\n\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This',\n",
       " 'is',\n",
       " 'an',\n",
       " 'example',\n",
       " 'sentence',\n",
       " 'with',\n",
       " 'words',\n",
       " 'that',\n",
       " 'may',\n",
       " 'not',\n",
       " 'be',\n",
       " 'in',\n",
       " 'the',\n",
       " 'shortlist',\n",
       " '.']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sacremoses import MosesTokenizer, MosesDetokenizer\n",
    "\n",
    "# Sample sentence\n",
    "sentence = \"This is an  example sentence with words that may not be in the shortlist.\"\n",
    "\n",
    "mt_en = MosesTokenizer(lang=\"en\")\n",
    "mt_fr = MosesTokenizer(lang=\"fr\")\n",
    "\n",
    "tokenized = mt_en.tokenize(sentence)\n",
    "tokenized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenizerWrapper:\n",
    "    def __init__(self, tokenizer_en, tokenizer_fr):\n",
    "        self.tokenizer_en = tokenizer_en\n",
    "        self.tokenizer_fr = tokenizer_fr\n",
    "    \n",
    "    def tokenize_function(self, examples):\n",
    "        return {\"tokenized_en\" : self.tokenizer_en.tokenize(examples[\"translation\"][\"en\"]),\n",
    "                \"tokenized_fr\" : self.tokenizer_fr.tokenize(examples[\"translation\"][\"fr\"])}\n",
    "    \n",
    "tokenizer_wrapper = TokenizerWrapper(mt_en, mt_fr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"processed_data/tokenized_train_data{}\".format(train_len)):\n",
    "    tokenized_train_data  = train_data.map(tokenizer_wrapper.tokenize_function, batched=False, num_proc=19, remove_columns=[\"translation\"])\n",
    "    tokenized_train_data.save_to_disk(\"processed_data/tokenized_train_data{}\".format(train_len))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=19): 100%|██████████| 1000/1000 [00:00<00:00, 1630.75 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 1000/1000 [00:00<00:00, 75041.67 examples/s]\n"
     ]
    }
   ],
   "source": [
    "    \n",
    "if not os.path.exists(\"processed_data/tokenized_val_data{}\".format(val_len)):\n",
    "    tokenized_val_data  = val_data.map(tokenizer_wrapper.tokenize_function, batched=False, num_proc=19, remove_columns=[\"translation\"])\n",
    "    tokenized_val_data.save_to_disk(\"processed_data/tokenized_val_data{}\".format(val_len))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_train_data = load_from_disk(\"processed_data/tokenized_train_data{}\".format(train_len))\n",
    "tokenized_val_data = load_from_disk(\"processed_data/tokenized_val_data{}\".format(val_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# df = pd.read_csv(\"data/Copy of Lexique383.csv\")\n",
    "# df.sort_values(by=\"9_freqfilms2\", ascending=False, inplace=True)\n",
    "# only_words_freq = df[[\"1_ortho\", \"9_freqfilms2\"]][:30000]\n",
    "# #drop index\n",
    "# only_words_freq.reset_index(drop=True, inplace=True)\n",
    "# #change column names\n",
    "# only_words_freq.columns = [\"word\", \"count\"]\n",
    "# only_words_freq.to_csv(\"data/unigram_freq_fr.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_frequent_english_words = bow_english[\"word\"][:30000].apply(lambda x: str(x)).tolist()\n",
    "most_frequent_english_words.append(\"UNK\")\n",
    "most_frequent_french_words = bow_french[\"word\"][:30000].apply(lambda x: str(x)).tolist()\n",
    "most_frequent_french_words.append(\"UNK\")\n",
    "tokenized_most_frequent_english_words = mt_en.tokenize(\" \".join(most_frequent_english_words))\n",
    "tokenized_most_frequent_french_words = mt_fr.tokenize(\" \".join(most_frequent_french_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class toIdTransform:\n",
    "    def __init__(self, most_frequent_words, tor):\n",
    "        self.most_frequent_words = most_frequent_words\n",
    "        self.word_to_id = {word: i for i, word in enumerate(most_frequent_words)}\n",
    "        self.tor = tor\n",
    "    \n",
    "    def __call__(self, tokenized):\n",
    "        return {\"ids_en\":[self.word_to_id.get(token, self.word_to_id[\"UNK\"]) for token in tokenized[\"tokenized_en\"]],\n",
    "                \"ids_fr\":[self.word_to_id.get(token, self.word_to_id[\"UNK\"]) for token in tokenized[\"tokenized_fr\"]]}\n",
    "    \n",
    "to_id_transform = toIdTransform(tokenized_most_frequent_english_words, torch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=19): 100%|██████████| 10000/10000 [00:03<00:00, 2828.22 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 315814.74 examples/s]\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(\"processed_data/id_train_data{}\".format(train_len)):\n",
    "    tokenized_train_data = tokenized_train_data.map(to_id_transform, batched=False, num_proc=19)\n",
    "    tokenized_train_data.save_to_disk(\"processed_data/id_train_data{}\".format(train_len))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=19): 100%|██████████| 1000/1000 [00:03<00:00, 310.84 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 1000/1000 [00:00<00:00, 113672.94 examples/s]\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(\"processed_data/id_val_data{}\".format(val_len)):\n",
    "    tokenized_val_data = tokenized_val_data.map(to_id_transform, batched=False, num_proc=19)\n",
    "    tokenized_val_data.save_to_disk(\"processed_data/id_val_data{}\".format(val_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['tokenized_en', 'tokenized_fr', 'ids_en', 'ids_fr'],\n",
      "    num_rows: 10000\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "tokenized_train_data = load_from_disk(\"processed_data/id_train_data{}\".format(train_len))\n",
    "tokenized_val_data = load_from_disk(\"processed_data/id_val_data{}\".format(val_len))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=19): 100%|██████████| 10000/10000 [00:00<00:00, 10796.45 examples/s]\n",
      "Map (num_proc=19): 100%|██████████| 1000/1000 [00:00<00:00, 3469.57 examples/s]\n"
     ]
    }
   ],
   "source": [
    "class to_tensor:\n",
    "    def __init__(self, tor):\n",
    "        self.tor = tor\n",
    "    \n",
    "    def __call__(self, ids):\n",
    "        return {\"ids_en\":self.tor.tensor(ids[\"ids_en\"]),\n",
    "                \"ids_fr\":self.tor.tensor(ids[\"ids_fr\"])}\n",
    "    \n",
    "tokenized_train_data = tokenized_train_data.map(to_tensor(torch), batched=False, num_proc=19)\n",
    "tokenized_val_data = tokenized_val_data.map(to_tensor(torch), batched=False, num_proc=19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_vector(ids, vocab_size):\n",
    "    vecs = torch.zeros(len(ids), vocab_size)\n",
    "    vecs[range(len(ids)), ids] = 1\n",
    "    return vecs\n",
    "\n",
    "class to_vec:\n",
    "    def __init__(self, tor, vocab_size, lang):\n",
    "        self.tor = tor\n",
    "        self.vocab_size = vocab_size\n",
    "        self.lang = lang\n",
    "\n",
    "    \n",
    "    def __call__(self, ids):\n",
    "        ids = self.tor.tensor(ids[\"ids_{}\".format(self.lang)])\n",
    "        vecs = self.tor.zeros(len(ids), self.vocab_size)\n",
    "        vecs[range(len(ids)), ids] = 1\n",
    "        return vecs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to_vec_transform_en = to_vec(torch, len(tokenized_most_frequent_english_words), \"en\")\n",
    "# to_vec_transform_fr = to_vec(torch, len(tokenized_most_frequent_french_words), \"fr\")\n",
    "# vecs_en = [\n",
    "\n",
    "# ]\n",
    "# vecs_fr = [\n",
    "\n",
    "# ]\n",
    "# for i in range(len(tokenized_train_data)):\n",
    "#     vecs_en.append(to_vec_transform_en(tokenized_train_data[i]))\n",
    "#     vecs_fr.append(to_vec_transform_fr(tokenized_train_data[i]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.fcnn import FCNN\n",
    "\n",
    "model = FCNN(len(tokenized_most_frequent_english_words), [10,10], 30000, \"cuda\" if torch.cuda.is_available() else \"cpu\",torch.nn.ReLU())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.2523,  0.2291, -0.2632,  ..., -0.0914,  0.2034,  0.0489],\n",
      "        [-0.2531,  0.2293, -0.2637,  ..., -0.0914,  0.2036,  0.0495],\n",
      "        [-0.2523,  0.2294, -0.2630,  ..., -0.0919,  0.2035,  0.0494],\n",
      "        [-0.2524,  0.2291, -0.2641,  ..., -0.0915,  0.2037,  0.0492]],\n",
      "       grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(tokenized_train_data)):\n",
    "\n",
    "    tensorized = torch.tensor(tokenized_train_data[i][\"ids_en\"]).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    result = model(to_vector(tensorized, len(tokenized_most_frequent_english_words)).to(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "    \n",
    "    print(result)\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.1419, -0.3834, -0.0322,  ..., -0.0372, -0.2247, -0.0284],\n",
      "         [-0.2926, -0.0300, -0.3833,  ..., -0.0826,  0.2095, -0.1092],\n",
      "         [-0.0649,  0.0589, -0.2137,  ..., -0.0094,  0.4779, -0.0252],\n",
      "         ...,\n",
      "         [-0.4876, -0.1592,  0.2788,  ...,  0.4932, -0.1656,  0.1080],\n",
      "         [-0.4504, -0.1160,  0.2697,  ...,  0.4124, -0.1343,  0.0948],\n",
      "         [-0.3177, -0.0596,  0.2093,  ...,  0.2608, -0.0819,  0.0626]],\n",
      "\n",
      "        [[ 0.2601,  0.1709,  0.0422,  ..., -0.5095,  0.2808, -0.0230],\n",
      "         [ 0.2512, -0.1345, -0.0254,  ..., -0.2761, -0.0472,  0.1546],\n",
      "         [ 0.1078,  0.0720, -0.3357,  ..., -0.1189, -0.0094, -0.0388],\n",
      "         ...,\n",
      "         [ 0.0559,  0.0588, -0.3089,  ..., -0.1702,  0.0875,  0.0595],\n",
      "         [-0.0696,  0.1379, -0.0028,  ..., -0.2555,  0.4171, -0.0326],\n",
      "         [ 0.3129,  0.0525,  0.3948,  ..., -0.2594,  0.1489,  0.2302]],\n",
      "\n",
      "        [[-0.2218,  0.3162, -0.1397,  ..., -0.4617,  0.0726, -0.1840],\n",
      "         [-0.0944,  0.1677,  0.1523,  ..., -0.2570,  0.0371,  0.2189],\n",
      "         [ 0.0196,  0.2004, -0.1359,  ...,  0.2424,  0.0364,  0.1597],\n",
      "         ...,\n",
      "         [-0.2715, -0.0658, -0.0490,  ...,  0.4423, -0.3118,  0.4416],\n",
      "         [-0.4504, -0.1160,  0.2697,  ...,  0.4124, -0.1343,  0.0948],\n",
      "         [-0.3177, -0.0596,  0.2093,  ...,  0.2608, -0.0819,  0.0626]]],\n",
      "       grad_fn=<SliceBackward0>) tensor([[[-0.2024,  0.1458, -0.1318,  ...,  0.1211, -0.1104, -0.2510],\n",
      "         [-0.0685, -0.1659, -0.1278,  ...,  0.2569, -0.1271, -0.3199],\n",
      "         [-0.2200,  0.0384, -0.1011,  ...,  0.1505, -0.0697, -0.1484]],\n",
      "\n",
      "        [[-0.1419, -0.3834, -0.0322,  ..., -0.0372, -0.2247, -0.0284],\n",
      "         [ 0.2601,  0.1709,  0.0422,  ..., -0.5095,  0.2808, -0.0230],\n",
      "         [-0.2218,  0.3162, -0.1397,  ..., -0.4617,  0.0726, -0.1840]]],\n",
      "       grad_fn=<StackBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_vocab_size, embedding_dim, enc_units):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_vocab_size (int): Taille du vocabulaire en entrée.\n",
    "            embedding_dim (int): Dimension de l'espace d'embedding.\n",
    "            enc_units (int): Nombre d'unités dans la couche GRU bidirectionnelle.\n",
    "        \"\"\"\n",
    "        super(Encoder, self).__init__()\n",
    "        self.enc_units = enc_units\n",
    "        self.input_vocab_size = input_vocab_size\n",
    "\n",
    "        # La couche d'embedding convertit les tokens en vecteurs.\n",
    "        self.embedding = nn.Embedding(input_vocab_size, embedding_dim)\n",
    "\n",
    "        # La couche GRU bidirectionnelle.\n",
    "        self.bigru = nn.GRU(embedding_dim, enc_units, bidirectional=True, batch_first=True)\n",
    "\n",
    "    def forward(self, tokens, state=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            tokens (torch.Tensor): Séquence de tokens en entrée.\n",
    "            state (torch.Tensor, optional): État initial de la couche GRU. Par défaut, None.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Séquence encodée.\n",
    "            torch.Tensor: État final de la couche GRU dans la direction arrière.\n",
    "        \"\"\"\n",
    "        # Embedding des tokens d'entrée.\n",
    "        vectors = self.embedding(tokens)\n",
    "\n",
    "        # La couche GRU bidirectionnelle traite la séquence d'embedding.\n",
    "        # La sortie a une forme de (batch, s, enc_units * 2)\n",
    "        # L'état a une forme de (num_layers * num_directions, batch, enc_units)\n",
    "        output, state = self.bigru(vectors, state)\n",
    "\n",
    "        # La sortie contient à la fois l'information des directions avant et arrière.\n",
    "        # Nous prenons seulement la partie correspondante à la direction arrière.\n",
    "        # Le slicing [:, :, self.enc_units:] prend les dimensions correspondant à la direction arrière.\n",
    "        backward_output = output[:, :, self.enc_units:]\n",
    "\n",
    "        # Nous renvoyons la séquence encodée et l'état final de la direction arrière.\n",
    "        return backward_output, state\n",
    "\n",
    "import torch.nn.utils.rnn as rnn_utils\n",
    "\n",
    "# Exemple d'utilisation\n",
    "input_vocab_size = 1000\n",
    "embedding_dim = 128\n",
    "enc_units = 256\n",
    "\n",
    "encoder = Encoder(input_vocab_size, embedding_dim, enc_units)\n",
    "\n",
    "# Supposons que tokens_batch soit une liste de séquences d'indices de mots.\n",
    "tokens_batch = [\n",
    "    torch.randint(0, input_vocab_size, (5,)),   # Longueur 5\n",
    "    torch.randint(0, input_vocab_size, (8,)),   # Longueur 8\n",
    "    torch.randint(0, input_vocab_size, (6,)),   # Longueur 6\n",
    "]\n",
    "\n",
    "# Ajout du padding\n",
    "padded_tokens_batch = rnn_utils.pad_sequence(tokens_batch, batch_first=True, padding_value=0)\n",
    "\n",
    "#print(padded_tokens_batch)\n",
    "\n",
    "# Utilisation de l'encodeur\n",
    "output, final_state = encoder(padded_tokens_batch)\n",
    "\n",
    "print(output, final_state )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(nn.Module):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.Wq = nn.Linear(units, units)\n",
    "        self.Wk = nn.Linear(units, units)\n",
    "        self.V = nn.Linear(units, 1, bias=False)\n",
    "\n",
    "    def forward(self, query, memory):\n",
    "        # Étapes 1 et 2 : Calcul des scores d'attention\n",
    "        score = self.V(torch.tanh(self.Wq(query) + self.Wk(memory)))\n",
    "\n",
    "        # Étape 3 : Calcul des poids d'attention\n",
    "        attention_weights = torch.softmax(score, dim=1)\n",
    "\n",
    "        # Étape 4 : Calcul du vecteur de contexte\n",
    "        context_vector = torch.bmm(attention_weights.transpose(1, 2), memory)\n",
    "\n",
    "        return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits shape: torch.Size([5, 1, 1000])\n",
      "New hidden state shape: torch.Size([1, 5, 512])\n",
      "Attention weights shape: torch.Size([5, 20, 1])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_vocab_size, embedding_dim, dec_units):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.dec_units = dec_units\n",
    "        self.output_vocab_size = output_vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        # Étape 1 : La couche d'embedding convertit les IDs de token en vecteurs\n",
    "        self.embedding = nn.Embedding(self.output_vocab_size, embedding_dim)\n",
    "\n",
    "        # Étape 2 : Le GRU garde une trace de ce qui a été généré jusqu'à présent.\n",
    "        self.gru = nn.GRU(embedding_dim, dec_units, batch_first=True)\n",
    "\n",
    "        # Étape 3 : La sortie du GRU sera la requête pour la couche d'attention.\n",
    "        self.attention = BahdanauAttention(dec_units)\n",
    "\n",
    "        # Étape 4 : Eqn. (3) : conversion de `ct` à `at`\n",
    "        self.Wc = nn.Linear(dec_units, dec_units, bias=False)\n",
    "\n",
    "        # Étape 5 : Cette couche entièrement connectée produit les logits pour chaque\n",
    "        # token de sortie.\n",
    "        self.fc = nn.Linear(dec_units, self.output_vocab_size)\n",
    "\n",
    "    def forward(self, x, hidden, enc_output):\n",
    "        # Étape 1 : Embedding\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        # Étape 2 : GRU\n",
    "        output, hidden = self.gru(x, hidden)\n",
    "\n",
    "        # Étape 3 : Attention\n",
    "        context_vector, attention_weights = self.attention(output, enc_output)\n",
    "\n",
    "        # Étape 4 : Eqn. (3) : conversion de `ct` à `at`\n",
    "        context_vector = torch.tanh(self.Wc(context_vector))\n",
    "\n",
    "        # Étape 5 : Logits\n",
    "        logits = self.fc(context_vector)\n",
    "\n",
    "        return logits, hidden, attention_weights\n",
    "    \n",
    "# Paramètres arbitraires pour le test\n",
    "output_vocab_size = 1000\n",
    "embedding_dim = 256\n",
    "dec_units = 512\n",
    "batch_size = 5\n",
    "seq_len = 20\n",
    "\n",
    "# Instanciation du décodeur\n",
    "decoder = Decoder(output_vocab_size, embedding_dim, dec_units)\n",
    "\n",
    "# Tenseurs d'entrée fictifs\n",
    "input_sequence = torch.randint(0, output_vocab_size, (batch_size, seq_len))\n",
    "hidden_state = torch.zeros((1, batch_size, dec_units))\n",
    "encoder_output = torch.randn(batch_size, seq_len, dec_units)\n",
    "\n",
    "# Passage avant (forward pass)\n",
    "logits, new_hidden, attention_weights = decoder(input_sequence, hidden_state, encoder_output)\n",
    "\n",
    "# Affichage des résultats\n",
    "print(\"Logits shape:\", logits.shape)\n",
    "print(\"New hidden state shape:\", new_hidden.shape)\n",
    "print(\"Attention weights shape:\", attention_weights.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Encoder.__init__() got an unexpected keyword argument 'input_vocab_size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/ikram/Desktop/M2/M2_ISI/mla/projet MLA/MLA_Project/main.ipynb Cellule 26\u001b[0m line \u001b[0;36m9\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ikram/Desktop/M2/M2_ISI/mla/projet%20MLA/MLA_Project/main.ipynb#X55sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m output_vocab_size \u001b[39m=\u001b[39m \u001b[39m1000\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ikram/Desktop/M2/M2_ISI/mla/projet%20MLA/MLA_Project/main.ipynb#X55sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m# Création d'une instance de la classe Encoder\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/ikram/Desktop/M2/M2_ISI/mla/projet%20MLA/MLA_Project/main.ipynb#X55sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m encoder \u001b[39m=\u001b[39m Encoder(input_vocab_size\u001b[39m=\u001b[39;49moutput_vocab_size, embedding_dim\u001b[39m=\u001b[39;49membedding_dim, enc_units\u001b[39m=\u001b[39;49mdec_units)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ikram/Desktop/M2/M2_ISI/mla/projet%20MLA/MLA_Project/main.ipynb#X55sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39m# Création d'une instance de la classe Decoder\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ikram/Desktop/M2/M2_ISI/mla/projet%20MLA/MLA_Project/main.ipynb#X55sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m decoder \u001b[39m=\u001b[39m Decoder(output_vocab_size\u001b[39m=\u001b[39moutput_vocab_size, embedding_dim\u001b[39m=\u001b[39membedding_dim, dec_units\u001b[39m=\u001b[39mdec_units)\n",
      "\u001b[0;31mTypeError\u001b[0m: Encoder.__init__() got an unexpected keyword argument 'input_vocab_size'"
     ]
    }
   ],
   "source": [
    "# Initialisation des dimensions fictives pour le test\n",
    "batch_size = 32\n",
    "seq_len = 10\n",
    "embedding_dim = 256\n",
    "dec_units = 512\n",
    "output_vocab_size = 1000\n",
    "\n",
    "# Création d'une instance de la classe Encoder\n",
    "encoder = Encoder(input_vocab_size=output_vocab_size, embedding_dim=embedding_dim, enc_units=dec_units)\n",
    "\n",
    "# Création d'une instance de la classe Decoder\n",
    "decoder = Decoder(output_vocab_size=output_vocab_size, embedding_dim=embedding_dim, dec_units=dec_units)\n",
    "\n",
    "# Tenseurs d'entrée fictifs pour l'encodeur\n",
    "input_sequence = torch.randint(0, 1000, (batch_size, seq_len))\n",
    "\n",
    "# Passage avant (forward pass) à travers l'encodeur\n",
    "enc_output, enc_state = encoder(input_sequence)\n",
    "\n",
    "# Affichage des résultats de l'encodeur\n",
    "print(\"Encoder Output shape:\", enc_output.shape)\n",
    "print(\"Encoder State shape:\", enc_state.shape)\n",
    "\n",
    "# Tenseurs d'entrée fictifs pour le décodeur\n",
    "input_sequence_decoder = torch.randint(0, 1000, (batch_size, seq_len))\n",
    "hidden_state_decoder = torch.zeros((1, batch_size, dec_units))\n",
    "\n",
    "# Passage avant (forward pass) à travers le décodeur\n",
    "logits_decoder, new_hidden_decoder, attention_weights_decoder = decoder(\n",
    "    input_sequence_decoder, hidden_state_decoder, enc_output)\n",
    "\n",
    "# Affichage des résultats du décodeur\n",
    "print(\"Logits shape (from decoder):\", logits_decoder.shape)\n",
    "print(\"New hidden state shape (from decoder):\", new_hidden_decoder.shape)\n",
    "print(\"Attention weights shape (from decoder):\", attention_weights_decoder.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.rnn import RNN\n",
    "\n",
    "encoder_rnn = RNN(len(tokenized_most_frequent_english_words),100,2,\"cuda\" if torch.cuda.is_available() else \"cpu\",torch.nn.Tanh())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4, 100])\n",
      "RNN(\n",
      "  (rnn): RNN(30001, 100, num_layers=2, batch_first=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "for x in tokenized_train_data:\n",
    "    tensorized = torch.tensor(x[\"ids_en\"]).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    result = encoder_rnn(to_vector(tensorized, len(tokenized_most_frequent_english_words)).to(\"cuda\" if torch.cuda.is_available() else \"cpu\").view(1,-1,len(tokenized_most_frequent_english_words)))\n",
    "    print(result[0].shape)\n",
    "    break\n",
    "print (encoder_rnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BLEU SCORE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1-EXISTANT NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8668778997501817\n"
     ]
    }
   ],
   "source": [
    "import nltk \n",
    "hypothesis=['Le','chat','se','trouve','sur','le','toit']\n",
    "reference=['Le','chat','se','trouve','sur','le','toit','bla']\n",
    "BLEUscore = nltk.translate.bleu_score.sentence_bleu([reference], hypothesis) \n",
    "print(BLEUscore)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2-SCORE PERSONNALISE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU Score: 0.392814650900513\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "import math\n",
    "\n",
    "def ngram_precision(candidate, reference, n):\n",
    "    candidate_ngrams = [tuple(candidate[i:i+n]) for i in range(len(candidate)-n+1)]\n",
    "    reference_ngrams = [tuple(reference[i:i+n]) for i in range(len(reference)-n+1)]\n",
    "\n",
    "    candidate_ngram_counts = collections.Counter(candidate_ngrams)\n",
    "    reference_ngram_counts = collections.Counter(reference_ngrams)\n",
    "\n",
    "    overlap_count = sum((candidate_ngram_counts & reference_ngram_counts).values())\n",
    "    total_count = max(1, len(candidate_ngrams))\n",
    "\n",
    "    precision = overlap_count / total_count\n",
    "    return precision\n",
    "\n",
    "def bleu_score(candidate, references, weights):\n",
    "    precisions = []\n",
    "\n",
    "    for i in range(len(weights)):\n",
    "        n = i + 1\n",
    "        precision = ngram_precision(candidate, references, n)\n",
    "        precisions.append(precision)\n",
    "\n",
    "    geometric_mean = math.exp(sum(weights[i] * math.log(p) for i, p in enumerate(precisions)))\n",
    "\n",
    "    brevity_penalty = min(1, len(candidate) / max(len(reference) for reference in references))\n",
    "\n",
    "    bleu = brevity_penalty * geometric_mean\n",
    "    return bleu\n",
    "\n",
    "# Exemple d'utilisation\n",
    "reference = ['the', 'quick', 'brown', 'fox', 'jumped', 'over', 'the', 'lazy', 'dog']\n",
    "candidate = ['the', 'fast', 'brown', 'fox', 'jumped', 'over', 'the', 'lazy', 'dog']\n",
    "\n",
    "candidate=['Le','chat','se','trou','s','le','to','bli']\n",
    "reference=['Le','chat','se','trouve','sur','le','toit']\n",
    "\n",
    "weights = [0.25,0.25,0.25] # You can adjust the weights based on your preference\n",
    "\n",
    "score = bleu_score(candidate, reference, weights)\n",
    "print(\"BLEU Score:\", score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlaproject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
