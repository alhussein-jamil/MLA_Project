{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "#auto reload \n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "from  prep_data import load_data\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Alhus\\.conda\\envs\\projetmla\\lib\\site-packages\\datasets\\table.py:1421: FutureWarning: promote has been superseded by mode='default'.\n",
      "  table = cls._concat_blocks(blocks, axis=0)\n"
     ]
    }
   ],
   "source": [
    "(train_data, train_dataloader), (val_data, val_dataloader), (bow_en, bow_fr) = load_data(10000,1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[   30,    25,     3,  ..., 30000, 30000, 30000],\n",
      "        [  223, 30000, 30000,  ...,    10,     0, 30000],\n",
      "        [  465, 30000,    11,  ...,    89,   808,     1],\n",
      "        ...,\n",
      "        [  126,     1,     0,  ..., 30000, 30000, 30000],\n",
      "        [ 1676, 30000,    13,  ..., 30000, 30000, 30000],\n",
      "        [   13,    85,    73,  ..., 30000, 30000, 30000]], dtype=torch.int16)\n"
     ]
    }
   ],
   "source": [
    "for x in train_dataloader:\n",
    "    print(x[\"english\"][\"idx\"])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def idx_to_word(idx: torch.Tensor, vocab: List):\n",
    "    return \" \".join(list(vocab[idx[(idx < len(vocab))]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'of and to a in for is on that by'"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx_to_word(torch.tensor([1,2,3,4,5,6,7,8,9,10,30000,30000,30000,30000,30000,30000,30000,30000]), data['bow']['english'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_vector(ids, vocab_size):\n",
    "    vecs = torch.zeros(len(ids), vocab_size)\n",
    "    vecs[range(len(ids)), ids] = 1\n",
    "    return vecs\n",
    "\n",
    "class to_vec:\n",
    "    def __init__(self, tor, vocab_size, lang):\n",
    "        self.tor = tor\n",
    "        self.vocab_size = vocab_size\n",
    "        self.lang = lang\n",
    "\n",
    "    \n",
    "    def __call__(self, ids):\n",
    "        ids = self.tor.tensor(ids[\"ids_{}\".format(self.lang)])\n",
    "        vecs = self.tor.zeros(len(ids), self.vocab_size)\n",
    "        vecs[range(len(ids)), ids] = 1\n",
    "        return vecs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BLEU SCORE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1-EXISTANT NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8668778997501817\n"
     ]
    }
   ],
   "source": [
    "import nltk \n",
    "hypothesis=['Le','chat','se','trouve','sur','le','toit']\n",
    "reference=['Le','chat','se','trouve','sur','le','toit','bla']\n",
    "BLEUscore = nltk.translate.bleu_score.sentence_bleu([reference], hypothesis) \n",
    "print(BLEUscore)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2-SCORE PERSONNALISE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU Score: 4.738137220537587\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "import math\n",
    "\n",
    "def ngram_precision(candidate, reference, n):\n",
    "    candidate_ngrams = [tuple(candidate[i:i+n]) for i in range(len(candidate)-n+1)]\n",
    "    reference_ngrams = [tuple(reference[i:i+n]) for i in range(len(reference)-n+1)]\n",
    "\n",
    "    candidate_ngram_counts = collections.Counter(candidate_ngrams)\n",
    "    reference_ngram_counts = collections.Counter(reference_ngrams)\n",
    "\n",
    "    overlap_count = sum((candidate_ngram_counts & reference_ngram_counts).values())\n",
    "    total_count = max(1, len(candidate_ngrams))\n",
    "\n",
    "    precision = overlap_count\n",
    "    return precision\n",
    "\n",
    "def bleu_score(candidate, references, weights):\n",
    "    precisions = []\n",
    "\n",
    "    for i in range(len(weights)):\n",
    "        n = i + 1\n",
    "        precision = ngram_precision(candidate, references, n)\n",
    "        precisions.append(precision)\n",
    "\n",
    "    # geometric_mean\n",
    "    bleu= math.exp(sum(weights[i] * math.log(p) for i, p in enumerate(precisions)))\n",
    "\n",
    "    # brevity_penalty = min(1, len(candidate) / max(len(reference) for reference in references))\n",
    "    # bleu = brevity_penalty * geometric_mean\n",
    "    \n",
    "    return bleu\n",
    "\n",
    "# Exemple d'utilisation\n",
    "reference = ['the', 'quick', 'brown', 'fox', 'jumped', 'over', 'the', 'lazy', 'dog']\n",
    "candidate = ['the', 'quick', 'brown', 'fox', 'jumped', 'over', 'the', 'lazy', 'dog']\n",
    "\n",
    "# candidate=['Le','chat','se','trou','s','le','to','bli']\n",
    "# reference=['Le','chat','se','trouve','sur','le','toit']\n",
    "\n",
    "weights = [0.25,0.25,0.25] # You can adjust the weights based on your preference\n",
    "\n",
    "score = bleu_score(candidate, reference, weights)\n",
    "print(\"BLEU Score:\", score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score BLEU: 45\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "\n",
    "def compute_bleu_score(reference, candidate, n=4):\n",
    "\n",
    "    # Calcul des n-grammes pour la référence et le candidat\n",
    "    reference_ngrams = [tuple(reference[i:i + n]) for i in range(len(reference) - n + 1)]\n",
    "    candidate_ngrams = [tuple(candidate[i:i + n]) for i in range(len(candidate) - n + 1)]\n",
    "\n",
    "    # Comptage des n-grammes dans la référence\n",
    "    reference_ngram_counts = collections.Counter(reference_ngrams)\n",
    "\n",
    "    # Comptage des n-grammes dans le candidat qui correspondent à la référence\n",
    "    matching_ngram_counts = sum(min(reference_ngram_counts[ngram], candidate_ngrams.count(ngram)) for ngram in set(candidate_ngrams))\n",
    "\n",
    "    # Calcul du score BLEU\n",
    "    precision = matching_ngram_counts \n",
    "\n",
    "    return precision \n",
    "\n",
    "# Exemple d'utilisation\n",
    "reference = \"the quick brown fox jumped over the lazy dog hey\"\n",
    "candidate = \"the quick brown fox jumped over the lazy dog hey\"\n",
    "\n",
    "# reference = ['the', 'quick', 'brown', 'fox', 'jumped', 'over', 'the', 'lazy', 'dog']\n",
    "# candidate = ['the', 'quick', 'brown', 'fox', 'jumped', 'over', 'the', 'lazy', 'dog']\n",
    "\n",
    "score_bleu = compute_bleu_score(reference, candidate)\n",
    "print(\"Score BLEU:\", score_bleu)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.Collecting torch\n",
      "  Downloading torch-2.1.1-cp310-cp310-win_amd64.whl.metadata (26 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\33785\\anaconda3\\envs\\projetmla\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\33785\\anaconda3\\envs\\projetmla\\lib\\site-packages (from torch) (4.8.0)\n",
      "Collecting sympy (from torch)\n",
      "  Using cached sympy-1.12-py3-none-any.whl (5.7 MB)\n",
      "Collecting networkx (from torch)\n",
      "  Using cached networkx-3.2.1-py3-none-any.whl.metadata (5.2 kB)\n",
      "Collecting jinja2 (from torch)\n",
      "  Using cached Jinja2-3.1.2-py3-none-any.whl (133 kB)\n",
      "Requirement already satisfied: fsspec in c:\\users\\33785\\anaconda3\\envs\\projetmla\\lib\\site-packages (from torch) (2023.10.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\33785\\anaconda3\\envs\\projetmla\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Collecting mpmath>=0.19 (from sympy->torch)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Downloading torch-2.1.1-cp310-cp310-win_amd64.whl (192.3 MB)\n",
      "   ---------------------------------------- 0.0/192.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/192.3 MB 640.0 kB/s eta 0:05:01\n",
      "   ---------------------------------------- 0.6/192.3 MB 5.4 MB/s eta 0:00:36\n",
      "   ---------------------------------------- 1.8/192.3 MB 11.5 MB/s eta 0:00:17\n",
      "    --------------------------------------- 2.9/192.3 MB 14.3 MB/s eta 0:00:14\n",
      "    --------------------------------------- 4.2/192.3 MB 16.7 MB/s eta 0:00:12\n",
      "   - -------------------------------------- 5.2/192.3 MB 17.6 MB/s eta 0:00:11\n",
      "   - -------------------------------------- 6.2/192.3 MB 18.0 MB/s eta 0:00:11\n",
      "   - -------------------------------------- 7.4/192.3 MB 19.7 MB/s eta 0:00:10\n",
      "   - -------------------------------------- 8.5/192.3 MB 20.1 MB/s eta 0:00:10\n",
      "   - -------------------------------------- 9.6/192.3 MB 20.5 MB/s eta 0:00:09\n",
      "   -- ------------------------------------- 10.6/192.3 MB 24.2 MB/s eta 0:00:08\n",
      "   -- ------------------------------------- 11.7/192.3 MB 24.2 MB/s eta 0:00:08\n",
      "   -- ------------------------------------- 12.9/192.3 MB 25.1 MB/s eta 0:00:08\n",
      "   -- ------------------------------------- 14.3/192.3 MB 24.2 MB/s eta 0:00:08\n",
      "   --- ------------------------------------ 15.6/192.3 MB 25.2 MB/s eta 0:00:08\n",
      "   --- ------------------------------------ 16.7/192.3 MB 25.2 MB/s eta 0:00:07\n",
      "   --- ------------------------------------ 17.5/192.3 MB 23.4 MB/s eta 0:00:08\n",
      "   --- ------------------------------------ 18.5/192.3 MB 23.4 MB/s eta 0:00:08\n",
      "   ---- ----------------------------------- 19.7/192.3 MB 24.2 MB/s eta 0:00:08\n",
      "   ---- ----------------------------------- 20.7/192.3 MB 23.4 MB/s eta 0:00:08\n",
      "   ---- ----------------------------------- 21.7/192.3 MB 23.4 MB/s eta 0:00:08\n",
      "   ---- ----------------------------------- 22.9/192.3 MB 22.6 MB/s eta 0:00:08\n",
      "   ----- ---------------------------------- 24.2/192.3 MB 23.4 MB/s eta 0:00:08\n",
      "   ----- ---------------------------------- 25.4/192.3 MB 23.4 MB/s eta 0:00:08\n",
      "   ----- ---------------------------------- 26.2/192.3 MB 22.6 MB/s eta 0:00:08\n",
      "   ----- ---------------------------------- 27.3/192.3 MB 23.4 MB/s eta 0:00:08\n",
      "   ----- ---------------------------------- 28.3/192.3 MB 24.2 MB/s eta 0:00:07\n",
      "   ------ --------------------------------- 29.5/192.3 MB 25.2 MB/s eta 0:00:07\n",
      "   ------ --------------------------------- 30.8/192.3 MB 25.2 MB/s eta 0:00:07\n",
      "   ------ --------------------------------- 31.8/192.3 MB 25.2 MB/s eta 0:00:07\n",
      "   ------ --------------------------------- 33.0/192.3 MB 25.2 MB/s eta 0:00:07\n",
      "   ------- -------------------------------- 34.3/192.3 MB 26.2 MB/s eta 0:00:07\n",
      "   ------- -------------------------------- 35.2/192.3 MB 26.2 MB/s eta 0:00:07\n",
      "   ------- -------------------------------- 36.4/192.3 MB 24.2 MB/s eta 0:00:07\n",
      "   ------- -------------------------------- 37.6/192.3 MB 24.2 MB/s eta 0:00:07\n",
      "   -------- ------------------------------- 38.8/192.3 MB 24.2 MB/s eta 0:00:07\n",
      "   -------- ------------------------------- 40.3/192.3 MB 25.1 MB/s eta 0:00:07\n",
      "   -------- ------------------------------- 41.5/192.3 MB 25.2 MB/s eta 0:00:06\n",
      "   -------- ------------------------------- 43.0/192.3 MB 26.2 MB/s eta 0:00:06\n",
      "   --------- ------------------------------ 44.3/192.3 MB 26.2 MB/s eta 0:00:06\n",
      "   --------- ------------------------------ 45.7/192.3 MB 26.2 MB/s eta 0:00:06\n",
      "   --------- ------------------------------ 46.9/192.3 MB 26.2 MB/s eta 0:00:06\n",
      "   ---------- ----------------------------- 48.4/192.3 MB 27.3 MB/s eta 0:00:06\n",
      "   ---------- ----------------------------- 49.5/192.3 MB 27.3 MB/s eta 0:00:06\n",
      "   ---------- ----------------------------- 50.2/192.3 MB 24.2 MB/s eta 0:00:06\n",
      "   ---------- ----------------------------- 51.3/192.3 MB 25.2 MB/s eta 0:00:06\n",
      "   ---------- ----------------------------- 52.3/192.3 MB 23.4 MB/s eta 0:00:06\n",
      "   ----------- ---------------------------- 53.2/192.3 MB 22.5 MB/s eta 0:00:07\n",
      "   ----------- ---------------------------- 55.0/192.3 MB 23.4 MB/s eta 0:00:06\n",
      "   ----------- ---------------------------- 56.3/192.3 MB 25.2 MB/s eta 0:00:06\n",
      "   ----------- ---------------------------- 57.7/192.3 MB 25.2 MB/s eta 0:00:06\n",
      "   ------------ --------------------------- 59.1/192.3 MB 25.2 MB/s eta 0:00:06\n",
      "   ------------ --------------------------- 60.3/192.3 MB 26.2 MB/s eta 0:00:06\n",
      "   ------------ --------------------------- 61.3/192.3 MB 26.2 MB/s eta 0:00:06\n",
      "   ------------- -------------------------- 62.5/192.3 MB 27.3 MB/s eta 0:00:05\n",
      "   ------------- -------------------------- 63.8/192.3 MB 28.5 MB/s eta 0:00:05\n",
      "   ------------- -------------------------- 65.3/192.3 MB 27.3 MB/s eta 0:00:05\n",
      "   ------------- -------------------------- 66.6/192.3 MB 28.5 MB/s eta 0:00:05\n",
      "   -------------- ------------------------- 67.8/192.3 MB 26.2 MB/s eta 0:00:05\n",
      "   -------------- ------------------------- 69.4/192.3 MB 28.5 MB/s eta 0:00:05\n",
      "   -------------- ------------------------- 70.4/192.3 MB 27.3 MB/s eta 0:00:05\n",
      "   -------------- ------------------------- 71.5/192.3 MB 26.2 MB/s eta 0:00:05\n",
      "   --------------- ------------------------ 73.1/192.3 MB 26.2 MB/s eta 0:00:05\n",
      "   --------------- ------------------------ 74.2/192.3 MB 28.4 MB/s eta 0:00:05\n",
      "   --------------- ------------------------ 75.2/192.3 MB 26.2 MB/s eta 0:00:05\n",
      "   --------------- ------------------------ 76.4/192.3 MB 27.3 MB/s eta 0:00:05\n",
      "   ---------------- ----------------------- 77.6/192.3 MB 25.2 MB/s eta 0:00:05\n",
      "   ---------------- ----------------------- 78.8/192.3 MB 25.1 MB/s eta 0:00:05\n",
      "   ---------------- ----------------------- 80.1/192.3 MB 27.3 MB/s eta 0:00:05\n",
      "   ---------------- ----------------------- 81.1/192.3 MB 27.3 MB/s eta 0:00:05\n",
      "   ----------------- ---------------------- 82.4/192.3 MB 27.3 MB/s eta 0:00:05\n",
      "   ----------------- ---------------------- 83.3/192.3 MB 26.2 MB/s eta 0:00:05\n",
      "   ----------------- ---------------------- 84.4/192.3 MB 25.2 MB/s eta 0:00:05\n",
      "   ----------------- ---------------------- 85.8/192.3 MB 26.2 MB/s eta 0:00:05\n",
      "   ------------------ --------------------- 87.1/192.3 MB 27.3 MB/s eta 0:00:04\n",
      "   ------------------ --------------------- 88.3/192.3 MB 27.3 MB/s eta 0:00:04\n",
      "   ------------------ --------------------- 89.8/192.3 MB 25.2 MB/s eta 0:00:05\n",
      "   ------------------ --------------------- 91.1/192.3 MB 25.2 MB/s eta 0:00:05\n",
      "   ------------------- -------------------- 91.7/192.3 MB 25.2 MB/s eta 0:00:04\n",
      "   ------------------- -------------------- 92.9/192.3 MB 25.2 MB/s eta 0:00:04\n",
      "   ------------------- -------------------- 93.8/192.3 MB 25.2 MB/s eta 0:00:04\n",
      "   ------------------- -------------------- 95.1/192.3 MB 27.3 MB/s eta 0:00:04\n",
      "   ------------------- -------------------- 95.9/192.3 MB 25.2 MB/s eta 0:00:04\n",
      "   -------------------- ------------------- 97.0/192.3 MB 24.3 MB/s eta 0:00:04\n",
      "   -------------------- ------------------- 98.1/192.3 MB 23.4 MB/s eta 0:00:05\n",
      "   -------------------- ------------------- 99.6/192.3 MB 24.2 MB/s eta 0:00:04\n",
      "   -------------------- ------------------ 100.5/192.3 MB 24.2 MB/s eta 0:00:04\n",
      "   -------------------- ------------------ 101.4/192.3 MB 23.4 MB/s eta 0:00:04\n",
      "   -------------------- ------------------ 102.6/192.3 MB 23.4 MB/s eta 0:00:04\n",
      "   --------------------- ----------------- 104.2/192.3 MB 22.6 MB/s eta 0:00:04\n",
      "   --------------------- ----------------- 105.6/192.3 MB 22.6 MB/s eta 0:00:04\n",
      "   --------------------- ----------------- 107.2/192.3 MB 26.2 MB/s eta 0:00:04\n",
      "   --------------------- ----------------- 108.5/192.3 MB 26.2 MB/s eta 0:00:04\n",
      "   ---------------------- ---------------- 109.9/192.3 MB 26.2 MB/s eta 0:00:04\n",
      "   ---------------------- ---------------- 111.1/192.3 MB 26.2 MB/s eta 0:00:04\n",
      "   ---------------------- ---------------- 112.4/192.3 MB 26.2 MB/s eta 0:00:04\n",
      "   ----------------------- --------------- 114.0/192.3 MB 27.3 MB/s eta 0:00:03\n",
      "   ----------------------- --------------- 115.1/192.3 MB 28.5 MB/s eta 0:00:03\n",
      "   ----------------------- --------------- 116.4/192.3 MB 26.2 MB/s eta 0:00:03\n",
      "   ----------------------- --------------- 117.7/192.3 MB 27.3 MB/s eta 0:00:03\n",
      "   ------------------------ -------------- 119.1/192.3 MB 26.2 MB/s eta 0:00:03\n",
      "   ------------------------ -------------- 120.6/192.3 MB 27.3 MB/s eta 0:00:03\n",
      "   ------------------------ -------------- 122.1/192.3 MB 28.5 MB/s eta 0:00:03\n",
      "   ------------------------ -------------- 123.3/192.3 MB 27.3 MB/s eta 0:00:03\n",
      "   ------------------------- ------------- 124.9/192.3 MB 28.5 MB/s eta 0:00:03\n",
      "   ------------------------- ------------- 125.8/192.3 MB 27.3 MB/s eta 0:00:03\n",
      "   ------------------------- ------------- 127.2/192.3 MB 27.3 MB/s eta 0:00:03\n",
      "   -------------------------- ------------ 128.5/192.3 MB 26.2 MB/s eta 0:00:03\n",
      "   -------------------------- ------------ 129.9/192.3 MB 26.2 MB/s eta 0:00:03\n",
      "   -------------------------- ------------ 131.0/192.3 MB 27.3 MB/s eta 0:00:03\n",
      "   -------------------------- ------------ 132.2/192.3 MB 24.2 MB/s eta 0:00:03\n",
      "   --------------------------- ----------- 133.8/192.3 MB 26.2 MB/s eta 0:00:03\n",
      "   --------------------------- ----------- 135.0/192.3 MB 25.1 MB/s eta 0:00:03\n",
      "   --------------------------- ----------- 136.2/192.3 MB 27.3 MB/s eta 0:00:03\n",
      "   --------------------------- ----------- 137.5/192.3 MB 27.3 MB/s eta 0:00:03\n",
      "   ---------------------------- ---------- 138.8/192.3 MB 28.4 MB/s eta 0:00:02\n",
      "   ---------------------------- ---------- 139.8/192.3 MB 27.3 MB/s eta 0:00:02\n",
      "   ---------------------------- ---------- 141.6/192.3 MB 26.2 MB/s eta 0:00:02\n",
      "   ----------------------------- --------- 143.1/192.3 MB 27.3 MB/s eta 0:00:02\n",
      "   ----------------------------- --------- 144.4/192.3 MB 28.4 MB/s eta 0:00:02\n",
      "   ----------------------------- --------- 145.5/192.3 MB 27.3 MB/s eta 0:00:02\n",
      "   ----------------------------- --------- 147.3/192.3 MB 27.3 MB/s eta 0:00:02\n",
      "   ------------------------------ -------- 148.4/192.3 MB 26.2 MB/s eta 0:00:02\n",
      "   ------------------------------ -------- 150.0/192.3 MB 28.4 MB/s eta 0:00:02\n",
      "   ------------------------------ -------- 150.8/192.3 MB 28.4 MB/s eta 0:00:02\n",
      "   ------------------------------ -------- 152.1/192.3 MB 26.2 MB/s eta 0:00:02\n",
      "   ------------------------------- ------- 153.5/192.3 MB 27.3 MB/s eta 0:00:02\n",
      "   ------------------------------- ------- 154.8/192.3 MB 27.3 MB/s eta 0:00:02\n",
      "   ------------------------------- ------- 156.3/192.3 MB 25.2 MB/s eta 0:00:02\n",
      "   ------------------------------- ------- 157.6/192.3 MB 26.2 MB/s eta 0:00:02\n",
      "   -------------------------------- ------ 158.6/192.3 MB 26.2 MB/s eta 0:00:02\n",
      "   -------------------------------- ------ 159.8/192.3 MB 25.2 MB/s eta 0:00:02\n",
      "   -------------------------------- ------ 161.0/192.3 MB 26.2 MB/s eta 0:00:02\n",
      "   -------------------------------- ------ 162.1/192.3 MB 25.2 MB/s eta 0:00:02\n",
      "   --------------------------------- ----- 163.4/192.3 MB 26.2 MB/s eta 0:00:02\n",
      "   --------------------------------- ----- 164.8/192.3 MB 25.1 MB/s eta 0:00:02\n",
      "   --------------------------------- ----- 166.1/192.3 MB 27.3 MB/s eta 0:00:01\n",
      "   --------------------------------- ----- 167.5/192.3 MB 24.2 MB/s eta 0:00:02\n",
      "   ---------------------------------- ---- 168.9/192.3 MB 27.3 MB/s eta 0:00:01\n",
      "   ---------------------------------- ---- 169.9/192.3 MB 26.2 MB/s eta 0:00:01\n",
      "   ---------------------------------- ---- 171.3/192.3 MB 26.2 MB/s eta 0:00:01\n",
      "   ---------------------------------- ---- 172.5/192.3 MB 27.3 MB/s eta 0:00:01\n",
      "   ----------------------------------- --- 174.0/192.3 MB 26.2 MB/s eta 0:00:01\n",
      "   ----------------------------------- --- 175.3/192.3 MB 27.3 MB/s eta 0:00:01\n",
      "   ----------------------------------- --- 176.6/192.3 MB 26.2 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 178.0/192.3 MB 27.3 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 179.4/192.3 MB 29.7 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 180.6/192.3 MB 27.3 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 182.1/192.3 MB 27.3 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 183.2/192.3 MB 25.2 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 184.7/192.3 MB 28.5 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 185.8/192.3 MB 27.3 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 187.2/192.3 MB 27.3 MB/s eta 0:00:01\n",
      "   --------------------------------------  188.5/192.3 MB 26.2 MB/s eta 0:00:01\n",
      "   --------------------------------------  189.8/192.3 MB 26.2 MB/s eta 0:00:01\n",
      "   --------------------------------------  191.2/192.3 MB 26.2 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.0/192.3 MB 25.2 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.3/192.3 MB 24.3 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.3/192.3 MB 24.3 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.3/192.3 MB 24.3 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.3/192.3 MB 24.3 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.3/192.3 MB 24.3 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.3/192.3 MB 24.3 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.3/192.3 MB 24.3 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.3/192.3 MB 24.3 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.3/192.3 MB 24.3 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.3/192.3 MB 24.3 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.3/192.3 MB 24.3 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.3/192.3 MB 24.3 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.3/192.3 MB 24.3 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.3/192.3 MB 24.3 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.3/192.3 MB 24.3 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.3/192.3 MB 24.3 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.3/192.3 MB 24.3 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.3/192.3 MB 24.3 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.3/192.3 MB 24.3 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.3/192.3 MB 24.3 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.3/192.3 MB 24.3 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.3/192.3 MB 24.3 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.3/192.3 MB 24.3 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.3/192.3 MB 24.3 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.3/192.3 MB 24.3 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.3/192.3 MB 24.3 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.3/192.3 MB 24.3 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.3/192.3 MB 24.3 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.3/192.3 MB 24.3 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.3/192.3 MB 24.3 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.3/192.3 MB 24.3 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.3/192.3 MB 24.3 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.3/192.3 MB 24.3 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.3/192.3 MB 24.3 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.3/192.3 MB 24.3 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.3/192.3 MB 24.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 192.3/192.3 MB 4.7 MB/s eta 0:00:00\n",
      "Using cached networkx-3.2.1-py3-none-any.whl (1.6 MB)\n",
      "Installing collected packages: mpmath, sympy, networkx, jinja2, torch\n",
      "Successfully installed jinja2-3.1.2 mpmath-1.3.0 networkx-3.2.1 sympy-1.12 torch-2.1.1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_dataloader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\33785\\MLA_Project\\main.ipynb Cell 33\u001b[0m line \u001b[0;36m4\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/33785/MLA_Project/main.ipynb#X45sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m \u001b[39m# Supposons que vous ayez un DataLoader nommé 'train_dataloader' pour l'ensemble d'entraînement\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/33785/MLA_Project/main.ipynb#X45sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m \u001b[39m# et un DataLoader nommé 'dev_dataloader' pour l'ensemble de développement\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/33785/MLA_Project/main.ipynb#X45sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m train_criterion \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mCrossEntropyLoss()  \u001b[39m# Vous pouvez utiliser d'autres critères en fonction de votre modèle\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/33785/MLA_Project/main.ipynb#X45sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m train_average_nll \u001b[39m=\u001b[39m compute_average_nll(model, train_dataloader, train_criterion)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/33785/MLA_Project/main.ipynb#X45sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m dev_criterion \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mCrossEntropyLoss()  \u001b[39m# Vous pouvez utiliser d'autres critères en fonction de votre modèle\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/33785/MLA_Project/main.ipynb#X45sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m dev_average_nll \u001b[39m=\u001b[39m compute_average_nll(model, dev_dataloader, dev_criterion)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_dataloader' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Exemple d'un modèle de langage simple pour l'illustration\n",
    "class SimpleLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
    "        super(SimpleLanguageModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        lstm_out, _ = self.lstm(embedded)\n",
    "        logits = self.linear(lstm_out)\n",
    "        return logits\n",
    "\n",
    "# Fonction pour calculer la NLL moyenne\n",
    "def compute_average_nll(model, dataloader, criterion):\n",
    "    total_loss = 0\n",
    "    total_tokens = 0\n",
    "\n",
    "    model.eval()  # Mettre le modèle en mode évaluation\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in dataloader:\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs.view(-1, outputs.size(-1)), targets.view(-1))\n",
    "            total_loss += loss.item()\n",
    "            total_tokens += targets.numel()\n",
    "\n",
    "    average_nll = total_loss / total_tokens\n",
    "\n",
    "    return average_nll\n",
    "\n",
    "# Exemple d'utilisation\n",
    "vocab_size = 10000\n",
    "embedding_dim = 100\n",
    "hidden_dim = 128\n",
    "model = SimpleLanguageModel(vocab_size, embedding_dim, hidden_dim)\n",
    "\n",
    "# Supposons que vous ayez un DataLoader nommé 'train_dataloader' pour l'ensemble d'entraînement\n",
    "# et un DataLoader nommé 'dev_dataloader' pour l'ensemble de développement\n",
    "train_criterion = nn.CrossEntropyLoss()  # Vous pouvez utiliser d'autres critères en fonction de votre modèle\n",
    "train_average_nll = compute_average_nll(model, train_dataloader, train_criterion)\n",
    "\n",
    "dev_criterion = nn.CrossEntropyLoss()  # Vous pouvez utiliser d'autres critères en fonction de votre modèle\n",
    "dev_average_nll = compute_average_nll(model, dev_dataloader, dev_criterion)\n",
    "\n",
    "print(\"Average NLL on training set:\", train_average_nll)\n",
    "print(\"Average NLL on development set:\", dev_average_nll)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs: tensor([1, 2, 3, 4, 5])\n",
      "Targets: tensor([ 6,  7,  8,  9, 10])\n",
      "Average NLL on training set: 9.161561965942383\n",
      "Average NLL on development set: 9.16692066192627\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Exemple de dataset\n",
    "class LanguageDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.data[idx])\n",
    "\n",
    "# Génération de données factices\n",
    "train_data = [\n",
    "    [1, 2, 3, 4, 5],\n",
    "    [6, 7, 8, 9, 10],\n",
    "    # Ajoutez d'autres exemples d'entraînement ici\n",
    "]\n",
    "\n",
    "dev_data = [\n",
    "    [11, 12, 13, 14, 15],\n",
    "    [16, 17, 18, 19, 20],\n",
    "    # Ajoutez d'autres exemples de développement ici\n",
    "]\n",
    "\n",
    "# Création des datasets\n",
    "train_dataset = LanguageDataset(train_data)\n",
    "dev_dataset = LanguageDataset(dev_data)\n",
    "\n",
    "# Création des DataLoader\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
    "dev_dataloader = DataLoader(dev_dataset, batch_size=2, shuffle=False)\n",
    "\n",
    "# Exemple d'utilisation du DataLoader\n",
    "for inputs, targets in train_dataloader:\n",
    "    print(\"Inputs:\", inputs)\n",
    "    print(\"Targets:\", targets)\n",
    "    break\n",
    "\n",
    "# Exemple d'un modèle de langage simple pour l'illustration\n",
    "class SimpleLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
    "        super(SimpleLanguageModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        lstm_out, _ = self.lstm(embedded)\n",
    "        logits = self.linear(lstm_out)\n",
    "        return logits\n",
    "\n",
    "# Fonction pour calculer la NLL moyenne\n",
    "def compute_average_nll(model, dataloader, criterion):\n",
    "    total_loss = 0\n",
    "    total_tokens = 0\n",
    "\n",
    "    model.eval()  # Mettre le modèle en mode évaluation\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in dataloader:\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs.view(-1, outputs.size(-1)), targets.view(-1))\n",
    "            total_loss += loss.item()\n",
    "            total_tokens += targets.numel()\n",
    "\n",
    "    average_nll = total_loss  #/ total_tokens\n",
    "\n",
    "    return average_nll\n",
    "\n",
    "# Exemple d'utilisation\n",
    "vocab_size = 10000\n",
    "embedding_dim = 100\n",
    "hidden_dim = 128\n",
    "model = SimpleLanguageModel(vocab_size, embedding_dim, hidden_dim)\n",
    "\n",
    "# Supposons que vous ayez un DataLoader nommé 'train_dataloader' pour l'ensemble d'entraînement\n",
    "# et un DataLoader nommé 'dev_dataloader' pour l'ensemble de développement\n",
    "train_criterion = nn.CrossEntropyLoss()  # Vous pouvez utiliser d'autres critères en fonction de votre modèle\n",
    "dev_criterion = nn.CrossEntropyLoss()  # Vous pouvez utiliser d'autres critères en fonction de votre modèle\n",
    "\n",
    "train_average_nll = compute_average_nll(model, train_dataloader, train_criterion)\n",
    "dev_average_nll = compute_average_nll(model, dev_dataloader, dev_criterion)\n",
    "\n",
    "print(\"Average NLL on training set:\", train_average_nll)\n",
    "print(\"Average NLL on development set:\", dev_average_nll)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def calculate_nll(sentences, lengths, model, device):\n",
    "    \"\"\"\n",
    "    Calculate Negative Log-Likelihood (NLL) for a set of sentences.\n",
    "\n",
    "    Args:\n",
    "    - sentences: List of tensorized sentences\n",
    "    - lengths: List of sentence lengths\n",
    "    - model: PyTorch model for language modeling\n",
    "    - device: Device on which to perform the calculations\n",
    "\n",
    "    Returns:\n",
    "    - nll: Average Negative Log-Likelihood\n",
    "    \"\"\"\n",
    "\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    nll_sum = 0.0\n",
    "    total_tokens = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(len(sentences)):\n",
    "            input_sentence = sentences[i].to(device)\n",
    "            target_sentence = input_sentence[:, 1:].contiguous()  # Shift the target by one position\n",
    "            input_lengths = lengths[i]\n",
    "\n",
    "            output = model(input_sentence[:, :-1], input_lengths)  # Model prediction\n",
    "            output_flattened = output.view(-1, output.size(-1))\n",
    "            target_flattened = target_sentence.view(-1)\n",
    "\n",
    "            # Mask out padding tokens\n",
    "            mask = (target_flattened != 0)  # Assuming 0 is the padding token index\n",
    "            output_masked = output_flattened[mask]\n",
    "            target_masked = target_flattened[mask]\n",
    "\n",
    "            nll_sum += F.cross_entropy(output_masked, target_masked, ignore_index=0, reduction='sum').item()\n",
    "            total_tokens += target_masked.size(0)\n",
    "\n",
    "    nll = nll_sum / total_tokens\n",
    "\n",
    "    return nll\n",
    "\n",
    "# Example usage:\n",
    "# Assume you have a PyTorch model called 'language_model'\n",
    "# and tokenized sentences in 'train_sentences' and 'dev_sentences'\n",
    "# with corresponding lengths in 'train_lengths' and 'dev_lengths'\n",
    "\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# nll_train = calculate_nll(train_sentences, train_lengths, language_model, device)\n",
    "# nll_dev = calculate_nll(dev_sentences, dev_lengths, language_model, device)\n",
    "\n",
    "# print(f\"Train NLL: {nll_train}\")\n",
    "# print(f\"Dev NLL: {nll_dev}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Modèle de langage fictif (à titre d'exemple)\n",
    "class LanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
    "        super(LanguageModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.rnn = nn.GRU(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        embedded = self.embedding(x)\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(embedded, lengths, batch_first=True, enforce_sorted=False)\n",
    "        output, _ = self.rnn(packed)\n",
    "        output, _ = nn.utils.rnn.pad_packed_sequence(output, batch_first=True)\n",
    "        output = self.fc(output)\n",
    "        return output\n",
    "\n",
    "# Exemple de données\n",
    "train_sentences = torch.tensor([[1, 2, 3, 4], [5, 6, 7, 8, 9]])\n",
    "train_lengths = [4, 5]\n",
    "\n",
    "dev_sentences = torch.tensor([[1, 2, 3], [4, 5, 6, 7]])\n",
    "dev_lengths = [3, 4]\n",
    "\n",
    "# Hyperparamètres fictifs\n",
    "vocab_size = 10\n",
    "embedding_dim = 16\n",
    "hidden_dim = 32\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Instancier le modèle\n",
    "model = LanguageModel(vocab_size, embedding_dim, hidden_dim)\n",
    "\n",
    "# Fonction de perte et optimiseur\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)  # Ignorer les indices de rembourrage\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Entraînement fictif (c'est un exemple simple, pas un entraînement complet)\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    for i in range(len(train_sentences)):\n",
    "        input_sentence = train_sentences[i].unsqueeze(0)\n",
    "        input_length = train_lengths[i]\n",
    "\n",
    "        target_sentence = input_sentence[:, 1:].contiguous()\n",
    "        output = model(input_sentence[:, :-1], [input_length])\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(output.view(-1, vocab_size), target_sentence.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Calcul de NLL pour les ensembles d'entraînement et de développement\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "nll_train = calculate_nll(train_sentences, train_lengths, model, device)\n",
    "nll_dev = calculate_nll(dev_sentences, dev_lengths, model, device)\n",
    "\n",
    "print(f\"Train NLL: {nll_train}\")\n",
    "print(f\"Dev NLL: {nll_dev}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlaproject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
