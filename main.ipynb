{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "#auto reload \n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os \n",
    "from datasets import load_dataset, load_from_disk\n",
    "import torch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    " \n",
    "bow_english = pd.read_csv(\"data/unigram_freq_en.csv\")\n",
    "bow_french = pd.read_csv(\"data/unigram_freq_fr.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "n, m = 30000, 30000\n",
    "\n",
    "bow_english = bow_english[:n]\n",
    "bow_french = bow_french[:m]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_len, val_len = 10000, 1000\n",
    "#train_len, val_len = None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nadia/anaconda3/envs/projetmla/lib/python3.10/site-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by mode='default'.\n",
      "  table = cls._concat_blocks(blocks, axis=0)\n"
     ]
    }
   ],
   "source": [
    "wmt14 = load_dataset(\"wmt14\", \"fr-en\", data_dir=\"data/\")\n",
    "\n",
    "# Accessing example data\n",
    "train_data = wmt14[\"train\"]\n",
    "val_data = wmt14[\"validation\"]\n",
    "\n",
    "if train_len is not None:\n",
    "    train_data = train_data.select(range(train_len))\n",
    "else :\n",
    "    train_len = len(train_data)\n",
    "if val_len is not None:\n",
    "    val_data = val_data.select(range(val_len))\n",
    "else : \n",
    "    val_len = len(val_data) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reprise de la session  ->  Resumption of the session \n",
      "\n",
      "\n",
      "Je déclare reprise la session du Parlement européen qui avait été interrompue le vendredi 17 décembre dernier et je vous renouvelle tous mes vux en espérant que vous avez passé de bonnes vacances.  ->  I declare resumed the session of the European Parliament adjourned on Friday 17 December 1999, and I would like once again to wish you a happy new year in the hope that you enjoyed a pleasant festive period. \n",
      "\n",
      "\n",
      "Comme vous avez pu le constater, le grand \"bogue de l'an 2000\" ne s'est pas produit. En revanche, les citoyens d'un certain nombre de nos pays ont été victimes de catastrophes naturelles qui ont vraiment été terribles.  ->  Although, as you will have seen, the dreaded 'millennium bug' failed to materialise, still the people in a number of countries suffered a series of natural disasters that truly were dreadful. \n",
      "\n",
      "\n",
      "Vous avez souhaité un débat à ce sujet dans les prochains jours, au cours de cette période de session.  ->  You have requested a debate on this subject in the course of the next few days, during this part-session. \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "examples = 4 \n",
    "#check some data and their translation\n",
    "for i, t in zip(range(examples), train_data):\n",
    "    ex = t[\"translation\"]\n",
    "    print(ex[\"fr\"], \" -> \", ex[\"en\"], \"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This',\n",
       " 'is',\n",
       " 'an',\n",
       " 'example',\n",
       " 'sentence',\n",
       " 'with',\n",
       " 'words',\n",
       " 'that',\n",
       " 'may',\n",
       " 'not',\n",
       " 'be',\n",
       " 'in',\n",
       " 'the',\n",
       " 'shortlist',\n",
       " '.']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sacremoses import MosesTokenizer, MosesDetokenizer\n",
    "\n",
    "# Sample sentence\n",
    "sentence = \"This is an  example sentence with words that may not be in the shortlist.\"\n",
    "\n",
    "mt_en = MosesTokenizer(lang=\"en\")\n",
    "mt_fr = MosesTokenizer(lang=\"fr\")\n",
    "\n",
    "tokenized = mt_en.tokenize(sentence)\n",
    "tokenized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenizerWrapper:\n",
    "    def __init__(self, tokenizer_en, tokenizer_fr):\n",
    "        self.tokenizer_en = tokenizer_en\n",
    "        self.tokenizer_fr = tokenizer_fr\n",
    "    \n",
    "    def preprocess_text(self, text):\n",
    "        # Add any additional preprocessing steps here\n",
    "        # For example, you can convert text to lowercase, remove special characters, etc.\n",
    "        # Modify this function based on your specific requirements.\n",
    "        return text.lower()\n",
    "\n",
    "    def tokenize_function(self, examples):\n",
    "        preprocessed_en = self.preprocess_text(examples[\"translation\"][\"en\"])\n",
    "        preprocessed_fr = self.preprocess_text(examples[\"translation\"][\"fr\"])\n",
    "\n",
    "        return {\"tokenized_en\": self.tokenizer_en.tokenize(preprocessed_en),\n",
    "                \"tokenized_fr\": self.tokenizer_fr.tokenize(preprocessed_fr)}\n",
    "\n",
    "tokenizer_wrapper = TokenizerWrapper(mt_en, mt_fr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=19): 100%|██████████| 10000/10000 [00:04<00:00, 2096.50 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 57883.13 examples/s]\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(\"processed_data/tokenized_train_data{}\".format(train_len)):\n",
    "    tokenized_train_data  = train_data.map(tokenizer_wrapper.tokenize_function, batched=False, num_proc=19, remove_columns=[\"translation\"])\n",
    "    tokenized_train_data.save_to_disk(\"processed_data/tokenized_train_data{}\".format(train_len))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=19): 100%|██████████| 1000/1000 [00:04<00:00, 233.75 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 1000/1000 [00:00<00:00, 135125.77 examples/s]\n"
     ]
    }
   ],
   "source": [
    "    \n",
    "if not os.path.exists(\"processed_data/tokenized_val_data{}\".format(val_len)):\n",
    "    tokenized_val_data  = val_data.map(tokenizer_wrapper.tokenize_function, batched=False, num_proc=19, remove_columns=[\"translation\"])\n",
    "    tokenized_val_data.save_to_disk(\"processed_data/tokenized_val_data{}\".format(val_len))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_train_data = load_from_disk(\"processed_data/tokenized_train_data{}\".format(train_len))\n",
    "tokenized_val_data = load_from_disk(\"processed_data/tokenized_val_data{}\".format(val_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_frequent_english_words = bow_english[\"word\"].apply(lambda x: str(x)).tolist()\n",
    "most_frequent_french_words = bow_french[\"word\"].apply(lambda x: str(x)).tolist()\n",
    "tokenized_most_frequent_english_words = mt_en.tokenize(\" \".join(most_frequent_english_words))[:n]\n",
    "tokenized_most_frequent_french_words = mt_fr.tokenize(\" \".join(most_frequent_french_words))[:m]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30000, 30000)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenized_most_frequent_french_words), len(tokenized_most_frequent_english_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class toIdTransform:\n",
    "    def __init__(self, most_frequent_words_en, most_frequent_words_fr, tor):\n",
    "        self.most_frequent_words_en = most_frequent_words_en\n",
    "        self.most_frequent_words_fr = most_frequent_words_fr\n",
    "        self.word_to_id_en = {word: i for i, word in enumerate(most_frequent_words_en)}\n",
    "        self.word_to_id_fr = {word: i for i, word in enumerate(most_frequent_words_fr)}\n",
    "        self.tor = tor\n",
    "    \n",
    "    def __call__(self, tokenized):\n",
    "        return {\"ids_en\":[self.word_to_id_en.get(token, len(self.most_frequent_words_en)) for token in tokenized[\"tokenized_en\"]],\n",
    "                \"ids_fr\":[self.word_to_id_fr.get(token, len(self.most_frequent_words_fr)) for token in tokenized[\"tokenized_fr\"]]}\n",
    "    \n",
    "to_id_transform = toIdTransform(tokenized_most_frequent_english_words, tokenized_most_frequent_french_words, torch.tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=19): 100%|██████████| 10000/10000 [00:11<00:00, 840.43 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 501861.08 examples/s]\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(\"processed_data/id_train_data{}\".format(train_len)):\n",
    "    tokenized_train_data = tokenized_train_data.map(to_id_transform, batched=False, num_proc=19)\n",
    "    tokenized_train_data.save_to_disk(\"processed_data/id_train_data{}\".format(train_len))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=19): 100%|██████████| 1000/1000 [00:11<00:00, 85.82 examples/s] \n",
      "Saving the dataset (1/1 shards): 100%|██████████| 1000/1000 [00:00<00:00, 59489.45 examples/s]\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(\"processed_data/id_val_data{}\".format(val_len)):\n",
    "    tokenized_val_data = tokenized_val_data.map(to_id_transform, batched=False, num_proc=19)\n",
    "    tokenized_val_data.save_to_disk(\"processed_data/id_val_data{}\".format(val_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_train_data = load_from_disk(\"processed_data/id_train_data{}\".format(train_len))\n",
    "tokenized_val_data = load_from_disk(\"processed_data/id_val_data{}\".format(val_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokenized_en': ['resumption', 'of', 'the', 'session'],\n",
       " 'tokenized_fr': ['reprise', 'de', 'la', 'session'],\n",
       " 'ids_en': [30000, 1, 0, 1497],\n",
       " 'ids_fr': [17227, 22350, 10301, 13488]}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tx = 30 \n",
    "Ty = 30\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_to_length(x, length, pad_value):\n",
    "    if len(x) < length:\n",
    "        return x + [pad_value] * (length - len(x))\n",
    "    else:\n",
    "        return x[:length]\n",
    "\n",
    "idx_train_tensor_en = torch.zeros((len(tokenized_train_data), Tx), dtype=torch.int16)\n",
    "idx_train_tensor_fr = torch.zeros((len(tokenized_train_data), Ty), dtype=torch.int16)\n",
    "\n",
    "for i, x in enumerate(tokenized_train_data):\n",
    "    idx_train_tensor_en[i] = torch.tensor(pad_to_length(x[\"ids_en\"], Tx, m+1))\n",
    "    idx_train_tensor_fr[i] = torch.tensor(pad_to_length(x[\"ids_fr\"], Ty, n+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([19328,   473,   109, 28542,  8254, 15510,   721, 30000, 10433,  8141,\n",
       "        30000,  2301,   109,   708,  4907,   492, 30000,  2635, 30000, 27924,\n",
       "           34,   109, 30000, 30000, 30000, 30001, 30001, 30001, 30001, 30001],\n",
       "       dtype=torch.int16)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx_train_tensor_fr[500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=19): 100%|██████████| 10000/10000 [00:00<00:00, 34597.88 examples/s]\n",
      "Map (num_proc=19): 100%|██████████| 1000/1000 [00:00<00:00, 6781.27 examples/s]\n"
     ]
    }
   ],
   "source": [
    "class to_tensor:\n",
    "    def __init__(self, tor):\n",
    "        self.tor = tor\n",
    "    \n",
    "    def __call__(self, ids):\n",
    "        return {\"ids_en\":self.tor.tensor(ids[\"ids_en\"]),\n",
    "                \"ids_fr\":self.tor.tensor(ids[\"ids_fr\"])}\n",
    "    \n",
    "tokenized_train_data = tokenized_train_data.map(to_tensor(torch), batched=False, num_proc=19)\n",
    "tokenized_val_data = tokenized_val_data.map(to_tensor(torch), batched=False, num_proc=19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_vector(ids, vocab_size):\n",
    "    vecs = torch.zeros(len(ids), vocab_size)\n",
    "    vecs[range(len(ids)), ids] = 1\n",
    "    return vecs\n",
    "\n",
    "class to_vec:\n",
    "    def __init__(self, tor, vocab_size, lang):\n",
    "        self.tor = tor\n",
    "        self.vocab_size = vocab_size\n",
    "        self.lang = lang\n",
    "\n",
    "    \n",
    "    def __call__(self, ids):\n",
    "        ids = self.tor.tensor(ids[\"ids_{}\".format(self.lang)])\n",
    "        vecs = self.tor.zeros(len(ids), self.vocab_size)\n",
    "        vecs[range(len(ids)), ids] = 1\n",
    "        return vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.fcnn import FCNN\n",
    "\n",
    "model = FCNN(len(tokenized_most_frequent_english_words), [10,10], 30000, \"cuda\" if torch.cuda.is_available() else \"cpu\",torch.nn.ReLU())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 30000])\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(tokenized_train_data)):\n",
    "\n",
    "    tensorized = torch.tensor(tokenized_train_data[i][\"ids_en\"]).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    result = model(to_vector(tensorized, len(tokenized_most_frequent_english_words)).to(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "    \n",
    "    print(result.shape)\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.rnn import RNN\n",
    "\n",
    "encoder_rnn = RNN(len(tokenized_most_frequent_english_words),100,2,\"cuda\" if torch.cuda.is_available() else \"cpu\",torch.nn.Tanh())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4, 100])\n",
      "RNN(\n",
      "  (rnn): RNN(30001, 100, num_layers=2, batch_first=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "for x in tokenized_train_data:\n",
    "    tensorized = torch.tensor(x[\"ids_en\"]).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    result = encoder_rnn(to_vector(tensorized, len(tokenized_most_frequent_english_words)).to(\"cuda\" if torch.cuda.is_available() else \"cpu\").view(1,-1,len(tokenized_most_frequent_english_words)))\n",
    "    print(result[0].shape)\n",
    "    break\n",
    "print (encoder_rnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.rnn import RNN\n",
    "from models.enco_deco import Encoder, Decoder  \n",
    "\n",
    "# Création de l'encodeur\n",
    "encoder = Encoder(len(tokenized_most_frequent_english_words), 256,512, 2, \"cuda\" if torch.cuda.is_available() else \"cpu\", 0.5)\n",
    "print(len(tokenized_most_frequent_english_words))\n",
    "print(encoder)\n",
    "\n",
    "for x in tokenized_train_data:\n",
    "    tensorized = torch.tensor(x[\"ids_en\"]).to(\"cuda\" if torch.cuda.is_available() else \"cpu\").long()\n",
    "    print(tensorized)\n",
    "    result = encoder(to_vector(tensorized, len(tokenized_most_frequent_english_words)).to(\"cuda\" if torch.cuda.is_available() else \"cpu\").view(1, -1, len(tokenized_most_frequent_english_words)))\n",
    "\n",
    "    print(result[0].shape)\n",
    "    break\n",
    "\n",
    "# Création du décodeur\n",
    "decoder = Decoder(len(tokenized_most_frequent_english_words), 100, 2, \"cuda\" if torch.cuda.is_available() else \"cpu\", torch.nn.Tanh())\n",
    "\n",
    "\n",
    "decoded_output, hidden_state = decoder(torch.tensor([1]), result[0], None)\n",
    "print(\"Decoded Output shape:\", decoded_output.shape)\n",
    "print(\"Hidden State shape:\", hidden_state.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BLEU SCORE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1-EXISTANT NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8668778997501817\n"
     ]
    }
   ],
   "source": [
    "import nltk \n",
    "hypothesis=['Le','chat','se','trouve','sur','le','toit']\n",
    "reference=['Le','chat','se','trouve','sur','le','toit','bla']\n",
    "BLEUscore = nltk.translate.bleu_score.sentence_bleu([reference], hypothesis) \n",
    "print(BLEUscore)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2-SCORE PERSONNALISE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU Score: 0.392814650900513\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "import math\n",
    "\n",
    "def ngram_precision(candidate, reference, n):\n",
    "    candidate_ngrams = [tuple(candidate[i:i+n]) for i in range(len(candidate)-n+1)]\n",
    "    reference_ngrams = [tuple(reference[i:i+n]) for i in range(len(reference)-n+1)]\n",
    "\n",
    "    candidate_ngram_counts = collections.Counter(candidate_ngrams)\n",
    "    reference_ngram_counts = collections.Counter(reference_ngrams)\n",
    "\n",
    "    overlap_count = sum((candidate_ngram_counts & reference_ngram_counts).values())\n",
    "    total_count = max(1, len(candidate_ngrams))\n",
    "\n",
    "    precision = overlap_count / total_count\n",
    "    return precision\n",
    "\n",
    "def bleu_score(candidate, references, weights):\n",
    "    precisions = []\n",
    "\n",
    "    for i in range(len(weights)):\n",
    "        n = i + 1\n",
    "        precision = ngram_precision(candidate, references, n)\n",
    "        precisions.append(precision)\n",
    "\n",
    "    geometric_mean = math.exp(sum(weights[i] * math.log(p) for i, p in enumerate(precisions)))\n",
    "\n",
    "    brevity_penalty = min(1, len(candidate) / max(len(reference) for reference in references))\n",
    "\n",
    "    bleu = brevity_penalty * geometric_mean\n",
    "    return bleu\n",
    "\n",
    "# Exemple d'utilisation\n",
    "reference = ['the', 'quick', 'brown', 'fox', 'jumped', 'over', 'the', 'lazy', 'dog']\n",
    "candidate = ['the', 'fast', 'brown', 'fox', 'jumped', 'over', 'the', 'lazy', 'dog']\n",
    "\n",
    "candidate=['Le','chat','se','trou','s','le','to','bli']\n",
    "reference=['Le','chat','se','trouve','sur','le','toit']\n",
    "\n",
    "weights = [0.25,0.25,0.25] # You can adjust the weights based on your preference\n",
    "\n",
    "score = bleu_score(candidate, reference, weights)\n",
    "print(\"BLEU Score:\", score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "\n",
    "#matmul tests: \n",
    "batch_size = 2 \n",
    "Tx = 3 \n",
    "hidden_size = 4\n",
    "a = torch.randint(0,10,(batch_size, Tx),dtype=torch.float)\n",
    "h = torch.randint(0,10,(batch_size, Tx, hidden_size),dtype=torch.float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[9., 4., 7.],\n",
       "        [0., 1., 6.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0., 3.],\n",
       "         [7., 1., 2.],\n",
       "         [9., 5., 0.],\n",
       "         [5., 6., 9.]],\n",
       "\n",
       "        [[8., 0., 7.],\n",
       "         [0., 0., 7.],\n",
       "         [9., 1., 0.],\n",
       "         [2., 3., 1.]]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h.swapaxes(1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 3, 4]), torch.Size([2, 3]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h.shape,a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 21.,  81., 101., 132.],\n",
       "        [ 42.,  42.,   1.,   9.]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(h.swapaxes(1,2) @ a.unsqueeze(2)).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlaproject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
