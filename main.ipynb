{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#auto reload \n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os \n",
    "from datasets import load_dataset, load_from_disk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    " \n",
    "bow_english = pd.read_csv(\"data/unigram_freq.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wmt14 = load_dataset(\"wmt14\", \"fr-en\", data_dir=\"data/\")\n",
    "\n",
    "# Accessing example data\n",
    "train_data = wmt14[\"train\"]\n",
    "val_data = wmt14[\"validation\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = 4 \n",
    "#check some data and their translation\n",
    "for i, t in zip(range(examples), train_data):\n",
    "    ex = t[\"translation\"]\n",
    "    print(ex[\"fr\"], \" -> \", ex[\"en\"], \"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sacremoses import MosesTokenizer, MosesDetokenizer\n",
    "\n",
    "# Sample sentence\n",
    "sentence = \"This is an  example sentence with words that may not be in the shortlist.\"\n",
    "\n",
    "mt  = MosesTokenizer(lang=\"en\")\n",
    "\n",
    "tokenized = mt.tokenize(sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenizerWrapper:\n",
    "    def __init__(self, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "    \n",
    "    def tokenize_function(self, examples):\n",
    "        return {\"tokenized\" : self.tokenizer.tokenize( examples[\"translation\"])}\n",
    "    \n",
    "tokenizer_wrapper = TokenizerWrapper(mt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"data/tokenized_train_data\"):\n",
    "    tokenized_train_data  = train_data.map(tokenizer_wrapper.tokenize_function, batched=True, num_proc=19, remove_columns=[\"translation\"])\n",
    "    tokenized_train_data.save_to_disk(\"data/tokenized_train_data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "if not os.path.exists(\"data/tokenized_val_data\"):\n",
    "    tokenized_val_data  = val_data.map(tokenizer_wrapper.tokenize_function, batched=True, num_proc=19, remove_columns=[\"translation\"])\n",
    "    tokenized_val_data.save_to_disk(\"data/tokenized_val_data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_train_data = load_from_disk(\"data/tokenized_train_data\")\n",
    "tokenized_val_data = load_from_disk(\"data/tokenized_val_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_frequent_english_words = bow_english[\"word\"][:30000].apply(lambda x: str(x)).tolist()\n",
    "most_frequent_english_words.append(\"UNK\")\n",
    "tokenized_most_frequent_english_words = mt.tokenize(\" \".join(most_frequent_english_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class toIdTransform:\n",
    "    def __init__(self, most_frequent_words):\n",
    "        self.most_frequent_words = most_frequent_words\n",
    "        self.word_to_id = {word: i for i, word in enumerate(most_frequent_words)}\n",
    "    \n",
    "    def __call__(self, tokenized):\n",
    "        return {\"ids\" : [self.word_to_id.get(word, self.word_to_id[\"UNK\"]) for word in tokenized[\"tokenized\"]]}\n",
    "    \n",
    "to_id_transform = toIdTransform(tokenized_most_frequent_english_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"data/id_train_data\"):\n",
    "    tokenized_train_data = tokenized_train_data.map(to_id_transform, batched=True, num_proc=19)\n",
    "    tokenized_train_data.save_to_disk(\"data/id_train_data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"data/id_val_data\"):\n",
    "    tokenized_val_data = tokenized_val_data.map(to_id_transform, batched=True, num_proc=19)\n",
    "    tokenized_val_data.save_to_disk(\"data/id_val_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_train_data = load_from_disk(\"data/tokenized_id_train_data\")\n",
    "tokenized_val_data = load_from_disk(\"data/tokenized_id_val_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "\n",
    "def id_to_vec(ids: torch.tensor, vocab_size: int):\n",
    "    vec = torch.zeros(ids.shape[0], vocab_size)\n",
    "    vec[torch.arange(ids.shape[0]), ids] = 1\n",
    "    return vec\n",
    "\n",
    "train_ids = torch.tensor(tokenized_train_data[\"ids\"])   \n",
    "val_ids = torch.tensor(tokenized_val_data[\"ids\"])\n",
    "\n",
    "train_vecs = id_to_vec(train_ids, len(tokenized_most_frequent_english_words))\n",
    "val_vecs = id_to_vec(val_ids, len(tokenized_most_frequent_english_words))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlaproject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
