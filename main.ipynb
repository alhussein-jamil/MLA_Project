{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#auto reload \n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Alhus\\.conda\\envs\\projetmla\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os \n",
    "from datasets import load_dataset, load_from_disk\n",
    "import torch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    " \n",
    "bow_english = pd.read_csv(\"data/unigram_freq_en.csv\")\n",
    "bow_french = pd.read_csv(\"data/unigram_freq_fr.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_len, val_len = 10000, 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Alhus\\.conda\\envs\\projetmla\\lib\\site-packages\\datasets\\table.py:1421: FutureWarning: promote has been superseded by mode='default'.\n",
      "  table = cls._concat_blocks(blocks, axis=0)\n"
     ]
    }
   ],
   "source": [
    "wmt14 = load_dataset(\"wmt14\", \"fr-en\", data_dir=\"data/\")\n",
    "\n",
    "# Accessing example data\n",
    "train_data = wmt14[\"train\"]\n",
    "val_data = wmt14[\"validation\"]\n",
    "\n",
    "if train_len is not None:\n",
    "    train_data = train_data.select(range(train_len))\n",
    "if val_len is not None:\n",
    "    val_data = val_data.select(range(val_len))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reprise de la session  ->  Resumption of the session \n",
      "\n",
      "\n",
      "Je déclare reprise la session du Parlement européen qui avait été interrompue le vendredi 17 décembre dernier et je vous renouvelle tous mes vux en espérant que vous avez passé de bonnes vacances.  ->  I declare resumed the session of the European Parliament adjourned on Friday 17 December 1999, and I would like once again to wish you a happy new year in the hope that you enjoyed a pleasant festive period. \n",
      "\n",
      "\n",
      "Comme vous avez pu le constater, le grand \"bogue de l'an 2000\" ne s'est pas produit. En revanche, les citoyens d'un certain nombre de nos pays ont été victimes de catastrophes naturelles qui ont vraiment été terribles.  ->  Although, as you will have seen, the dreaded 'millennium bug' failed to materialise, still the people in a number of countries suffered a series of natural disasters that truly were dreadful. \n",
      "\n",
      "\n",
      "Vous avez souhaité un débat à ce sujet dans les prochains jours, au cours de cette période de session.  ->  You have requested a debate on this subject in the course of the next few days, during this part-session. \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "examples = 4 \n",
    "#check some data and their translation\n",
    "for i, t in zip(range(examples), train_data):\n",
    "    ex = t[\"translation\"]\n",
    "    print(ex[\"fr\"], \" -> \", ex[\"en\"], \"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This',\n",
       " 'is',\n",
       " 'an',\n",
       " 'example',\n",
       " 'sentence',\n",
       " 'with',\n",
       " 'words',\n",
       " 'that',\n",
       " 'may',\n",
       " 'not',\n",
       " 'be',\n",
       " 'in',\n",
       " 'the',\n",
       " 'shortlist',\n",
       " '.']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sacremoses import MosesTokenizer, MosesDetokenizer\n",
    "\n",
    "# Sample sentence\n",
    "sentence = \"This is an  example sentence with words that may not be in the shortlist.\"\n",
    "\n",
    "mt_en = MosesTokenizer(lang=\"en\")\n",
    "mt_fr = MosesTokenizer(lang=\"fr\")\n",
    "\n",
    "tokenized = mt_en.tokenize(sentence)\n",
    "tokenized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenizerWrapper:\n",
    "    def __init__(self, tokenizer_en, tokenizer_fr):\n",
    "        self.tokenizer_en = tokenizer_en\n",
    "        self.tokenizer_fr = tokenizer_fr\n",
    "    \n",
    "    def tokenize_function(self, examples):\n",
    "        return {\"tokenized_en\" : self.tokenizer_en.tokenize(examples[\"translation\"][\"en\"]),\n",
    "                \"tokenized_fr\" : self.tokenizer_fr.tokenize(examples[\"translation\"][\"fr\"])}\n",
    "    \n",
    "tokenizer_wrapper = TokenizerWrapper(mt_en, mt_fr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 591480.14 examples/s]\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(\"processed_data/tokenized_train_data{}\".format(train_len)):\n",
    "    tokenized_train_data  = train_data.map(tokenizer_wrapper.tokenize_function, batched=False, num_proc=19, remove_columns=[\"translation\"])\n",
    "    tokenized_train_data.save_to_disk(\"processed_data/tokenized_train_data{}\".format(train_len))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 1000/1000 [00:00<00:00, 183349.54 examples/s]\n"
     ]
    }
   ],
   "source": [
    "    \n",
    "if not os.path.exists(\"processed_data/tokenized_val_data{}\".format(val_len)):\n",
    "    tokenized_val_data  = val_data.map(tokenizer_wrapper.tokenize_function, batched=False, num_proc=19, remove_columns=[\"translation\"])\n",
    "    tokenized_val_data.save_to_disk(\"processed_data/tokenized_val_data{}\".format(val_len))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_train_data = load_from_disk(\"processed_data/tokenized_train_data{}\".format(train_len))\n",
    "tokenized_val_data = load_from_disk(\"processed_data/tokenized_val_data{}\".format(val_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# df = pd.read_csv(\"data/Copy of Lexique383.csv\")\n",
    "# df.sort_values(by=\"9_freqfilms2\", ascending=False, inplace=True)\n",
    "# only_words_freq = df[[\"1_ortho\", \"9_freqfilms2\"]][:30000]\n",
    "# #drop index\n",
    "# only_words_freq.reset_index(drop=True, inplace=True)\n",
    "# #change column names\n",
    "# only_words_freq.columns = [\"word\", \"count\"]\n",
    "# only_words_freq.to_csv(\"data/unigram_freq_fr.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_frequent_english_words = bow_english[\"word\"][:30000].apply(lambda x: str(x)).tolist()\n",
    "most_frequent_english_words.append(\"UNK\")\n",
    "most_frequent_french_words = bow_french[\"word\"][:30000].apply(lambda x: str(x)).tolist()\n",
    "most_frequent_french_words.append(\"UNK\")\n",
    "tokenized_most_frequent_english_words = mt_en.tokenize(\" \".join(most_frequent_english_words))\n",
    "tokenized_most_frequent_french_words = mt_fr.tokenize(\" \".join(most_frequent_french_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class toIdTransform:\n",
    "    def __init__(self, most_frequent_words, tor):\n",
    "        self.most_frequent_words = most_frequent_words\n",
    "        self.word_to_id = {word: i for i, word in enumerate(most_frequent_words)}\n",
    "        self.tor = tor\n",
    "    \n",
    "    def __call__(self, tokenized):\n",
    "        return {\"ids_en\":[self.word_to_id.get(token, self.word_to_id[\"UNK\"]) for token in tokenized[\"tokenized_en\"]],\n",
    "                \"ids_fr\":[self.word_to_id.get(token, self.word_to_id[\"UNK\"]) for token in tokenized[\"tokenized_fr\"]]}\n",
    "    \n",
    "to_id_transform = toIdTransform(tokenized_most_frequent_english_words, torch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=19): 100%|██████████| 10000/10000 [00:06<00:00, 1440.73 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10000/10000 [00:00<00:00, 623076.83 examples/s]\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(\"processed_data/id_train_data{}\".format(train_len)):\n",
    "    tokenized_train_data = tokenized_train_data.map(to_id_transform, batched=False, num_proc=19)\n",
    "    tokenized_train_data.save_to_disk(\"processed_data/id_train_data{}\".format(train_len))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=19): 100%|██████████| 1000/1000 [00:06<00:00, 149.82 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 1000/1000 [00:00<00:00, 140814.61 examples/s]\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(\"processed_data/id_val_data{}\".format(val_len)):\n",
    "    tokenized_val_data = tokenized_val_data.map(to_id_transform, batched=False, num_proc=19)\n",
    "    tokenized_val_data.save_to_disk(\"processed_data/id_val_data{}\".format(val_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_train_data = load_from_disk(\"processed_data/id_train_data{}\".format(train_len))\n",
    "tokenized_val_data = load_from_disk(\"processed_data/id_val_data{}\".format(val_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=19): 100%|██████████| 10000/10000 [00:06<00:00, 1634.52 examples/s]\n",
      "Map (num_proc=19): 100%|██████████| 1000/1000 [00:05<00:00, 180.50 examples/s]\n"
     ]
    }
   ],
   "source": [
    "class to_tensor:\n",
    "    def __init__(self, tor):\n",
    "        self.tor = tor\n",
    "    \n",
    "    def __call__(self, ids):\n",
    "        return {\"ids_en\":self.tor.tensor(ids[\"ids_en\"]),\n",
    "                \"ids_fr\":self.tor.tensor(ids[\"ids_fr\"])}\n",
    "    \n",
    "tokenized_train_data = tokenized_train_data.map(to_tensor(torch), batched=False, num_proc=19)\n",
    "tokenized_val_data = tokenized_val_data.map(to_tensor(torch), batched=False, num_proc=19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_vec(ids, vocab_size):\n",
    "    vecs = torch.zeros(len(ids), vocab_size)\n",
    "    vecs[range(len(ids)), ids] = 1\n",
    "    return vecs\n",
    "\n",
    "class to_vec:\n",
    "    def __init__(self, tor, vocab_size, lang):\n",
    "        self.tor = tor\n",
    "        self.vocab_size = vocab_size\n",
    "        self.lang = lang\n",
    "\n",
    "    \n",
    "    def __call__(self, ids):\n",
    "        ids = self.tor.tensor(ids[\"ids_{}\".format(self.lang)])\n",
    "        vecs = self.tor.zeros(len(ids), self.vocab_size)\n",
    "        vecs[range(len(ids)), ids] = 1\n",
    "        return vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to_vec_transform_en = to_vec(torch, len(tokenized_most_frequent_english_words), \"en\")\n",
    "# to_vec_transform_fr = to_vec(torch, len(tokenized_most_frequent_french_words), \"fr\")\n",
    "# vecs_en = [\n",
    "\n",
    "# ]\n",
    "# vecs_fr = [\n",
    "\n",
    "# ]\n",
    "# for i in range(len(tokenized_train_data)):\n",
    "#     vecs_en.append(to_vec_transform_en(tokenized_train_data[i]))\n",
    "#     vecs_fr.append(to_vec_transform_fr(tokenized_train_data[i]))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlaproject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
